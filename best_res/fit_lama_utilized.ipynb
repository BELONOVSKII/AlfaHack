{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../automl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.automl.model.lama import TabularLamaUtilized\n",
    "from src.automl.loggers import configure_root_logger\n",
    "from src.automl.constants import create_ml_data_dir\n",
    "from src.automl.model.metrics import RocAuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_ml_data_dir()\n",
    "configure_root_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune and fit [LightAutoMLUtilized](https://github.com/sb-ai-lab/LightAutoML) \n",
    "Find the best parameters for LightAutoML and then fit the model on these parameters. Parameters optimization is performed based on the 5-fold stratified cross-validation and the final fit is performed on the same folds. Out of fold predictions are saved for further stacking/blending. \n",
    "[TabularLamaUtilized](https://github.com/dertty/automl/blob/hack/src/automl/model/lama/default_lama.py) implementation from [automl](https://github.com/dertty/automl/tree/hack) is used.\n",
    "\n",
    "**Unfortunately**, in LightAutoML training and tuning is performed simultaneously, hence it is impossible to save best LightAutoML parameters and then initialize model with these parameters for inference. The solution is to save model file (*joblib* format) and then use this model for inference. If necessary, we can provide this file together with the oof predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/\")\n",
    "RANDOM_SEED = 77\n",
    "N_JOBS = 16\n",
    "CONFIG_FILE = Path(\"../configs/config.yaml\")\n",
    "\n",
    "with CONFIG_FILE.open(\"r\") as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(DATA_PATH / \"train_preproc_2.parquet\")\n",
    "cat_columns = df_train.drop(columns=[\"target\", \"id\"]).select_dtypes(int).columns.values.tolist()\n",
    "X_train, y_train = df_train[cfg[\"selected_features\"] + cat_columns], df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = RocAuc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit + Tune\n",
    "\n",
    "**Important**: It is nearly impossible to fully reproduce LightAutoML trianing, because it strongly depends on the harware, resources utilization and timeout. To reproduce the results we can provide the saved file of a fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-08 13:27:24,414] - [   START    ] - Fitting TabularLamaUtilized\n",
      "[13:27:24] Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n",
      "[13:27:24] - time: 7200.00 seconds\n",
      "[13:27:24] - CPU: 16 cores\n",
      "[13:27:24] - memory: 16 GB\n",
      "\n",
      "[13:27:24] \u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
      "\n",
      "[13:27:24] ==================================================\n",
      "[13:27:24] Start 0 automl preset configuration:\n",
      "[13:27:24] \u001b[1mconf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n",
      "[13:27:24] Stdout logging level is INFO.\n",
      "[13:27:24] Task: binary\n",
      "\n",
      "[13:27:24] Start automl preset with listed constraints:\n",
      "[13:27:24] - time: 7200.00 seconds\n",
      "[13:27:24] - CPU: 16 cores\n",
      "[13:27:24] - memory: 16 GB\n",
      "\n",
      "[13:27:24] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[13:27:35] Layer \u001b[1m1\u001b[0m train process start. Time left 7188.70 secs\n",
      "[13:27:48] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[13:28:33] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7879458135561579\u001b[0m\n",
      "[13:28:33] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[13:28:33] Time left 7131.06 secs\n",
      "\n",
      "[13:28:45] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[13:29:34] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.807851439415669\u001b[0m\n",
      "[13:29:34] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:29:34] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[13:29:34] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[13:34:34] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[13:34:34] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[13:35:37] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8124643025361464\u001b[0m\n",
      "[13:35:37] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:35:37] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[13:36:32] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8069578372829955\u001b[0m\n",
      "[13:36:32] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:36:32] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[13:41:34] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[13:41:34] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[13:43:37] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8078924384298085\u001b[0m\n",
      "[13:43:37] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:43:37] Time left 6227.33 secs\n",
      "\n",
      "[13:43:37] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[13:43:37] Blending: optimization starts with equal weights and score \u001b[1m0.8099334769656599\u001b[0m\n",
      "[13:43:42] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8128415584666869\u001b[0m, weights = \u001b[1m[0.         0.09734569 0.7682273  0.05446735 0.07995959]\u001b[0m\n",
      "[13:43:48] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8128669857951489\u001b[0m, weights = \u001b[1m[0.         0.15655714 0.7124255  0.05992553 0.07109183]\u001b[0m\n",
      "[13:43:54] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8128670833538414\u001b[0m, weights = \u001b[1m[0.         0.15073249 0.7175082  0.06035306 0.07140625]\u001b[0m\n",
      "[13:43:59] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8128671098265244\u001b[0m, weights = \u001b[1m[0.         0.15062104 0.7176024  0.06036098 0.07141562]\u001b[0m\n",
      "[13:44:05] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.8128671098265244\u001b[0m, weights = \u001b[1m[0.         0.15062104 0.7176024  0.06036098 0.07141562]\u001b[0m\n",
      "[13:44:05] Blending: no score update. Terminated\n",
      "\n",
      "[13:44:05] \u001b[1mAutoml preset training completed in 1000.96 seconds\u001b[0m\n",
      "\n",
      "[13:44:05] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.15062 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.71760 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.06036 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.07142 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[13:44:05] ==================================================\n",
      "[13:44:05] Start 1 automl preset configuration:\n",
      "[13:44:05] \u001b[1mconf_1_sel_type_1.yml\u001b[0m, random state: {'reader_params': {'random_state': 43}, 'nn_params': {'random_state': 43}, 'general_params': {'return_all_predictions': False}}\n",
      "[13:44:05] Stdout logging level is INFO.\n",
      "[13:44:05] Task: binary\n",
      "\n",
      "[13:44:05] Start automl preset with listed constraints:\n",
      "[13:44:05] - time: 6199.02 seconds\n",
      "[13:44:05] - CPU: 16 cores\n",
      "[13:44:05] - memory: 16 GB\n",
      "\n",
      "[13:44:05] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[13:44:16] Layer \u001b[1m1\u001b[0m train process start. Time left 6188.11 secs\n",
      "[13:44:29] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[13:45:06] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7876202250782859\u001b[0m\n",
      "[13:45:06] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[13:45:06] Time left 6137.87 secs\n",
      "\n",
      "[13:45:16] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:45:27] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[13:46:18] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8076547164783409\u001b[0m\n",
      "[13:46:18] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:46:18] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[13:51:23] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[13:51:23] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[13:52:31] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8123549376598648\u001b[0m\n",
      "[13:52:31] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:52:31] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[13:53:27] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8070730620089563\u001b[0m\n",
      "[13:53:27] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:53:27] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[13:58:42] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[13:58:42] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:00:30] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8076847717802356\u001b[0m\n",
      "[14:00:30] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:00:30] Time left 5213.60 secs\n",
      "\n",
      "[14:00:30] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:00:31] Blending: optimization starts with equal weights and score \u001b[1m0.8101190890226051\u001b[0m\n",
      "[14:00:36] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8129737483855946\u001b[0m, weights = \u001b[1m[0.         0.10132591 0.69546604 0.08396929 0.11923878]\u001b[0m\n",
      "[14:00:42] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8129891202605231\u001b[0m, weights = \u001b[1m[0.         0.14416976 0.66524607 0.07825273 0.11233138]\u001b[0m\n",
      "[14:00:47] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8129891206823985\u001b[0m, weights = \u001b[1m[0.         0.1441697  0.6652458  0.0782527  0.11233176]\u001b[0m\n",
      "[14:00:53] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8129891200495851\u001b[0m, weights = \u001b[1m[0.         0.1441697  0.66524583 0.0782527  0.11233176]\u001b[0m\n",
      "[14:00:53] Blending: no score update. Terminated\n",
      "\n",
      "[14:00:53] \u001b[1mAutoml preset training completed in 1008.01 seconds\u001b[0m\n",
      "\n",
      "[14:00:53] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.14417 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.66525 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.07825 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.11233 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[14:00:53] ==================================================\n",
      "[14:00:53] Start 2 automl preset configuration:\n",
      "[14:00:53] \u001b[1mconf_2_select_mode_1_no_typ.yml\u001b[0m, random state: {'reader_params': {'random_state': 44}, 'nn_params': {'random_state': 44}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:00:53] Stdout logging level is INFO.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:00:53] Task: binary\n",
      "\n",
      "[14:00:53] Start automl preset with listed constraints:\n",
      "[14:00:53] - time: 5190.98 seconds\n",
      "[14:00:53] - CPU: 16 cores\n",
      "[14:00:53] - memory: 16 GB\n",
      "\n",
      "[14:00:53] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[14:00:54] Layer \u001b[1m1\u001b[0m train process start. Time left 5190.17 secs\n",
      "[14:01:07] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:01:56] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7878750314025083\u001b[0m\n",
      "[14:01:56] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:01:56] Time left 5128.30 secs\n",
      "\n",
      "[14:02:07] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:02:19] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:03:08] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8076809265966576\u001b[0m\n",
      "[14:03:08] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:03:08] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:08:09] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:08:09] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:10:06] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.812175810513778\u001b[0m\n",
      "[14:10:06] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:10:06] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:10:55] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.806652519181102\u001b[0m\n",
      "[14:10:55] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:10:55] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:15:57] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:15:57] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:17:43] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8074051022804657\u001b[0m\n",
      "[14:17:43] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:17:43] Time left 4180.95 secs\n",
      "\n",
      "[14:17:43] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:17:43] Blending: optimization starts with equal weights and score \u001b[1m0.8095102302854604\u001b[0m\n",
      "[14:17:49] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.812476779765568\u001b[0m, weights = \u001b[1m[0.         0.10875404 0.89124596 0.         0.        ]\u001b[0m\n",
      "[14:17:55] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8125672611286026\u001b[0m, weights = \u001b[1m[0.         0.21042496 0.78957504 0.         0.        ]\u001b[0m\n",
      "[14:18:01] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8125672611286026\u001b[0m, weights = \u001b[1m[0.         0.21042496 0.78957504 0.         0.        ]\u001b[0m\n",
      "[14:18:01] Blending: no score update. Terminated\n",
      "\n",
      "[14:18:01] \u001b[1mAutoml preset training completed in 1027.68 seconds\u001b[0m\n",
      "\n",
      "[14:18:01] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.21042 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.78958 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) \n",
      "\n",
      "[14:18:01] ==================================================\n",
      "[14:18:01] Start 3 automl preset configuration:\n",
      "[14:18:01] \u001b[1mconf_3_sel_type_1_no_inter_lgbm.yml\u001b[0m, random state: {'reader_params': {'random_state': 45}, 'nn_params': {'random_state': 45}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:18:01] Stdout logging level is INFO.\n",
      "[14:18:01] Task: binary\n",
      "\n",
      "[14:18:01] Start automl preset with listed constraints:\n",
      "[14:18:01] - time: 4163.27 seconds\n",
      "[14:18:01] - CPU: 16 cores\n",
      "[14:18:01] - memory: 16 GB\n",
      "\n",
      "[14:18:01] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[14:18:12] Layer \u001b[1m1\u001b[0m train process start. Time left 4152.16 secs\n",
      "[14:18:25] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:19:05] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7876062487672695\u001b[0m\n",
      "[14:19:05] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:19:05] Time left 4099.36 secs\n",
      "\n",
      "[14:19:16] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:19:17] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:20:05] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8081465128001653\u001b[0m\n",
      "[14:20:05] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:20:05] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:25:07] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:25:07] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:26:09] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8122458029166284\u001b[0m\n",
      "[14:26:09] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:26:09] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:27:01] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8069951970414997\u001b[0m\n",
      "[14:27:01] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:27:01] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:32:02] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:32:02] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:33:46] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8074204663506988\u001b[0m\n",
      "[14:33:46] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:33:46] Time left 3217.66 secs\n",
      "\n",
      "[14:33:46] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:33:47] Blending: optimization starts with equal weights and score \u001b[1m0.8098117900573794\u001b[0m\n",
      "[14:33:52] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8126887609351313\u001b[0m, weights = \u001b[1m[0.         0.12196411 0.76422    0.05569938 0.05811656]\u001b[0m\n",
      "[14:33:58] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8127327393917386\u001b[0m, weights = \u001b[1m[0.         0.19578423 0.6999689  0.0510165  0.05323046]\u001b[0m\n",
      "[14:34:03] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8127328005109411\u001b[0m, weights = \u001b[1m[0.         0.19544856 0.70026094 0.05103779 0.05325267]\u001b[0m\n",
      "[14:34:09] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8127328005109411\u001b[0m, weights = \u001b[1m[0.         0.19544856 0.70026094 0.05103779 0.05325267]\u001b[0m\n",
      "[14:34:09] Blending: no score update. Terminated\n",
      "\n",
      "[14:34:09] \u001b[1mAutoml preset training completed in 967.79 seconds\u001b[0m\n",
      "\n",
      "[14:34:09] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.19545 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.70026 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.05104 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.05325 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[14:34:09] ==================================================\n",
      "[14:34:09] Start 4 automl preset configuration:\n",
      "[14:34:09] \u001b[1mconf_4_sel_type_0_no_int.yml\u001b[0m, random state: {'reader_params': {'random_state': 46}, 'nn_params': {'random_state': 46}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:34:09] Stdout logging level is INFO.\n",
      "[14:34:09] Task: binary\n",
      "\n",
      "[14:34:09] Start automl preset with listed constraints:\n",
      "[14:34:09] - time: 3195.46 seconds\n",
      "[14:34:09] - CPU: 16 cores\n",
      "[14:34:09] - memory: 16 GB\n",
      "\n",
      "[14:34:09] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[14:34:19] Layer \u001b[1m1\u001b[0m train process start. Time left 3184.81 secs\n",
      "[14:34:21] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:35:11] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7877428157491996\u001b[0m\n",
      "[14:35:11] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:35:11] Time left 3133.00 secs\n",
      "\n",
      "[14:35:12] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:36:02] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8083088889605534\u001b[0m\n",
      "[14:36:02] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:36:02] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:41:03] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:41:03] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:42:01] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8121930536698967\u001b[0m\n",
      "[14:42:01] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:42:02] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:42:54] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8067263183241322\u001b[0m\n",
      "[14:42:54] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:42:54] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:48:05] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:48:05] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:49:53] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8075557040558934\u001b[0m\n",
      "[14:49:53] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:49:53] Time left 2250.78 secs\n",
      "\n",
      "[14:49:53] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:49:53] Blending: optimization starts with equal weights and score \u001b[1m0.8097974977616682\u001b[0m\n",
      "[14:49:59] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8126527270256249\u001b[0m, weights = \u001b[1m[0.         0.13179252 0.75211096 0.         0.11609649]\u001b[0m\n",
      "[14:50:05] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8126923457161023\u001b[0m, weights = \u001b[1m[0.         0.20192623 0.6923848  0.         0.10568899]\u001b[0m\n",
      "[14:50:10] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8126923457161023\u001b[0m, weights = \u001b[1m[0.         0.20192623 0.6923848  0.         0.10568899]\u001b[0m\n",
      "[14:50:10] Blending: no score update. Terminated\n",
      "\n",
      "[14:50:10] \u001b[1mAutoml preset training completed in 961.74 seconds\u001b[0m\n",
      "\n",
      "[14:50:10] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.20193 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.69238 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.10569 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[14:50:10] ==================================================\n",
      "[14:50:10] Start 5 automl preset configuration:\n",
      "[14:50:10] \u001b[1mconf_5_sel_type_1_tuning_full.yml\u001b[0m, random state: {'reader_params': {'random_state': 47}, 'nn_params': {'random_state': 47}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:50:10] Stdout logging level is INFO.\n",
      "[14:50:10] Task: binary\n",
      "\n",
      "[14:50:10] Start automl preset with listed constraints:\n",
      "[14:50:10] - time: 2233.69 seconds\n",
      "[14:50:10] - CPU: 16 cores\n",
      "[14:50:10] - memory: 16 GB\n",
      "\n",
      "[14:50:10] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[14:50:21] Layer \u001b[1m1\u001b[0m train process start. Time left 2222.97 secs\n",
      "[14:50:34] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:51:15] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7876537068523974\u001b[0m\n",
      "[14:51:15] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:51:15] Time left 2168.66 secs\n",
      "\n",
      "[14:51:25] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:51:37] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:52:25] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.807618347019667\u001b[0m\n",
      "[14:52:25] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:52:25] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:52:25] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:53:12] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8072341434336592\u001b[0m\n",
      "[14:53:12] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:53:12] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:54:20] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8108961112072435\u001b[0m\n",
      "[14:54:20] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:54:20] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:55:06] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8076843957310269\u001b[0m\n",
      "[14:55:06] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:55:06] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:55:51] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8087874717221331\u001b[0m\n",
      "[14:55:51] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:55:52] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:56:38] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8059814828087797\u001b[0m\n",
      "[14:56:38] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:56:38] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:57:31] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8109994857688703\u001b[0m\n",
      "[14:57:31] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:57:32] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:57:32] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:58:27] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8109994857688703\u001b[0m\n",
      "[14:58:27] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:58:27] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:59:22] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8064440283978829\u001b[0m\n",
      "[14:59:22] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:59:22] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:59:22] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:00:36] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8072320004646941\u001b[0m\n",
      "[15:00:36] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:00:36] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:01:13] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.804571623331485\u001b[0m\n",
      "[15:01:13] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:01:14] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:02:45] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8067270453736961\u001b[0m\n",
      "[15:02:45] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:02:45] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:03:14] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.7901660634549782\u001b[0m\n",
      "[15:03:14] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:03:14] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:03:33] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.7810105699276635\u001b[0m\n",
      "[15:03:33] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:03:33] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:04:54] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8064033760071408\u001b[0m\n",
      "[15:04:54] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:04:55] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[15:04:55] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:06:48] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8072410041827885\u001b[0m\n",
      "[15:06:48] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:06:48] Time left 1236.34 secs\n",
      "\n",
      "[15:06:48] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[15:06:48] Blending: optimization starts with equal weights and score \u001b[1m0.809089679547442\u001b[0m\n",
      "[15:06:53] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8115833047228669\u001b[0m, weights = \u001b[1m[0.         0.16094835 0.71104413 0.         0.12800752]\u001b[0m\n",
      "[15:06:59] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8116223401178322\u001b[0m, weights = \u001b[1m[0.         0.2333537  0.6496679  0.         0.11697838]\u001b[0m\n",
      "[15:07:04] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8116223404342386\u001b[0m, weights = \u001b[1m[0.         0.2326738  0.6506886  0.         0.11663754]\u001b[0m\n",
      "[15:07:10] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8116223404342386\u001b[0m, weights = \u001b[1m[0.         0.2326738  0.6506886  0.         0.11663754]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:07:10] Blending: no score update. Terminated\n",
      "\n",
      "[15:07:10] \u001b[1mAutoml preset training completed in 1019.65 seconds\u001b[0m\n",
      "\n",
      "[15:07:10] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.23267 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.65069 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.11664 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[15:07:10] ==================================================\n",
      "[15:07:10] Start 6 automl preset configuration:\n",
      "[15:07:10] \u001b[1mconf_6_sel_type_1_tuning_full_no_int_lgbm.yml\u001b[0m, random state: {'reader_params': {'random_state': 48}, 'nn_params': {'random_state': 48}, 'general_params': {'return_all_predictions': False}}\n",
      "[15:07:10] Stdout logging level is INFO.\n",
      "[15:07:10] Task: binary\n",
      "\n",
      "[15:07:10] Start automl preset with listed constraints:\n",
      "[15:07:10] - time: 1214.01 seconds\n",
      "[15:07:10] - CPU: 16 cores\n",
      "[15:07:10] - memory: 16 GB\n",
      "\n",
      "[15:07:10] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[15:07:21] Layer \u001b[1m1\u001b[0m train process start. Time left 1203.45 secs\n",
      "[15:07:33] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[15:08:20] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7876164851526267\u001b[0m\n",
      "[15:08:20] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[15:08:20] Time left 1144.31 secs\n",
      "\n",
      "[15:08:31] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[15:08:31] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[15:09:18] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8075631078114314\u001b[0m\n",
      "[15:09:18] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[15:09:18] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 84.19 secs\n",
      "[15:10:44] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[15:10:44] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[15:11:41] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8112458412954796\u001b[0m\n",
      "[15:11:41] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[15:11:41] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[15:12:39] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8072399829278489\u001b[0m\n",
      "[15:12:39] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:12:39] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[15:17:45] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[15:17:45] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:19:12] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8075759614064681\u001b[0m\n",
      "[15:19:12] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:19:12] Time left 491.80 secs\n",
      "\n",
      "[15:19:12] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[15:19:12] Blending: optimization starts with equal weights and score \u001b[1m0.8095417395282795\u001b[0m\n",
      "[15:19:18] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8120607766337133\u001b[0m, weights = \u001b[1m[0.         0.13727422 0.6602281  0.11111946 0.09137826]\u001b[0m\n",
      "[15:19:24] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8120843265637914\u001b[0m, weights = \u001b[1m[0.         0.19312277 0.61389506 0.12853554 0.06444657]\u001b[0m\n",
      "[15:19:29] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8120866143414969\u001b[0m, weights = \u001b[1m[0.         0.19033775 0.61427057 0.14336202 0.05202963]\u001b[0m\n",
      "[15:19:35] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8120872337600924\u001b[0m, weights = \u001b[1m[0.         0.18603529 0.6130106  0.14744331 0.05351083]\u001b[0m\n",
      "[15:19:40] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.8120872357112662\u001b[0m, weights = \u001b[1m[0.         0.18603855 0.6130038  0.1474459  0.05351177]\u001b[0m\n",
      "[15:19:40] \u001b[1mAutoml preset training completed in 750.21 seconds\u001b[0m\n",
      "\n",
      "[15:19:40] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.18604 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.61300 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.14745 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.05351 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[15:19:40] ==================================================\n",
      "[15:19:40] Blending: optimization starts with equal weights and score \u001b[1m0.8140943091619701\u001b[0m\n",
      "[15:19:48] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8142515575847478\u001b[0m, weights = \u001b[1m[0.1882052  0.25352237 0.16428886 0.15741904 0.15952097 0.\n",
      " 0.07704351]\u001b[0m\n",
      "[15:19:57] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8142543761872092\u001b[0m, weights = \u001b[1m[0.22066888 0.2560528  0.1515602  0.14921875 0.14579372 0.\n",
      " 0.07670563]\u001b[0m\n",
      "[15:20:05] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8142544259685095\u001b[0m, weights = \u001b[1m[0.22041632 0.25575975 0.15138674 0.15028381 0.14562686 0.\n",
      " 0.07652655]\u001b[0m\n",
      "[15:20:13] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8142544259685095\u001b[0m, weights = \u001b[1m[0.22041632 0.25575975 0.15138674 0.15028381 0.14562686 0.\n",
      " 0.07652655]\u001b[0m\n",
      "[15:20:13] Blending: no score update. Terminated\n",
      "\n",
      "[2024-11-08 15:20:13,135] - [    END     ] - Fitting TabularLamaUtilized\n",
      "0.8142544259685095\n"
     ]
    }
   ],
   "source": [
    "model = TabularLamaUtilized(n_jobs=N_JOBS, task=\"classification\")\n",
    "model.tune(X_train, y_train, metric, timeout=60 * 60, categorical_features=cat_columns)\n",
    "oof = model.fit(X_train, y_train, categorical_features=cat_columns)\n",
    "\n",
    "print(metric(y_train, oof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, load the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load(DATA_PATH / \"models\" / \"lamau_81425_full_dataset\" / \"lamau_81425_full_dataset.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model file, parameters, test and oof predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lamau_81425_full_dataset\"\n",
    "MODEL_DIR = DATA_PATH / \"models\" / MODEL_NAME\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res[MODEL_NAME] = oof[:, 1]\n",
    "res.to_csv(MODEL_DIR / \"oof.csv\", index=False)\n",
    "joblib.dump(model, MODEL_DIR / f\"{MODEL_NAME}.joblib\")\n",
    "\n",
    "with (MODEL_DIR / \"params.yaml\").open(\"w\") as f:\n",
    "    yaml.dump(model.params, f)\n",
    "\n",
    "with (MODEL_DIR / \"score.txt\").open(\"w\") as f:\n",
    "    print(\"OOF:\", metric(y_train, oof), file=f)\n",
    "    \n",
    "test = pd.read_parquet(DATA_PATH / \"test_preproc_2.parquet\")\n",
    "test[\"target\"] = model.predict(test[cfg[\"selected_features\"] + cat_columns])[:, 1]\n",
    "test[['id', 'target']].to_csv(MODEL_DIR / f'{MODEL_NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_kernel",
   "language": "python",
   "name": "base_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
