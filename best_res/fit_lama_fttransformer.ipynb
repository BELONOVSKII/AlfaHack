{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../automl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.automl.model.lama import TabularLamaNN\n",
    "from src.automl.loggers import configure_root_logger\n",
    "from src.automl.constants import create_ml_data_dir\n",
    "from src.automl.model.metrics import RocAuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_ml_data_dir()\n",
    "configure_root_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune and fit [LightAutoML FTTransformer](https://github.com/sb-ai-lab/LightAutoML) \n",
    "Fit LightAutoML FTTransformer on the 5-fold stratified cross-validation. Out of fold predictions are saved for further stacking/blending. \n",
    "[TabularLamaNN](https://github.com/dertty/automl/blob/hack/src/automl/model/lama/nn_lama.py) implementation from [automl](https://github.com/dertty/automl/tree/hack) is used.\n",
    "\n",
    "**Unfortunately**, in LightAutoML training and tuning is performed simultaneously, hence it is impossible to save best LightAutoML parameters and then initialize model with these parameters for inference. The solution is to save model file (*joblib* format) and then use this model for inference. If necessary, we can provide this file together with the oof predictions.\n",
    "\n",
    "**Note:** GPU is required to fit tabular NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/\")\n",
    "RANDOM_SEED = 77\n",
    "N_JOBS = 16\n",
    "CONFIG_FILE = Path(\"../configs/config.yaml\")\n",
    "\n",
    "with CONFIG_FILE.open(\"r\") as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(DATA_PATH / \"train_preproc_2.parquet\")\n",
    "cat_columns = df_train.drop(columns=[\"target\", \"id\"]).select_dtypes(int).columns.values.tolist()\n",
    "X_train, y_train = df_train[cfg[\"selected_features\"] + cat_columns], df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = RocAuc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit + Tune\n",
    "\n",
    "**Important**: It is nearly impossible to fully reproduce LightAutoML trianing, because it strongly depends on the harware, resources utilization and timeout. To reproduce the results we can provide the saved file of a fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-07 12:09:14,787] - [   START    ] - Fitting TabularLamaNN_fttransformer\n",
      "[12:09:14] Stdout logging level is DEBUG.\n",
      "[12:09:14] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[12:09:14] Task: binary\n",
      "\n",
      "[12:09:14] Start automl preset with listed constraints:\n",
      "[12:09:14] - time: 14400.00 seconds\n",
      "[12:09:14] - CPU: 16 cores\n",
      "[12:09:14] - memory: 16 GB\n",
      "\n",
      "[12:09:14] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[12:09:30] Feats was rejected during automatic roles guess: []\n",
      "[12:09:30] Layer \u001b[1m1\u001b[0m train process start. Time left 14384.16 secs\n",
      "[12:09:40] number of text features: 0 \n",
      "[12:09:40] number of categorical features: 4 \n",
      "[12:09:40] number of continuous features: 58 \n",
      "[12:09:40] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m ...\n",
      "[12:09:40] Training params: {'num_workers': 0, 'pin_memory': False, 'max_length': 256, 'is_snap': False, 'input_bn': False, 'max_emb_size': 256, 'bert_name': None, 'pooling': 'cls', 'device': device(type='cuda', index=0), 'use_cont': True, 'use_cat': True, 'use_text': False, 'lang': 'en', 'deterministic': True, 'multigpu': False, 'random_state': 42, 'model': 'fttransformer', 'model_with_emb': False, 'path_to_save': None, 'verbose_inside': None, 'verbose': 1, 'n_epochs': 50, 'snap_params': {'k': 3, 'early_stopping': True, 'patience': 10, 'swa': True}, 'bs': 1024, 'emb_dropout': 0.1, 'emb_ratio': 3, 'opt': 'Adam', 'opt_params': {'lr': 0.0003, 'weight_decay': 0}, 'sch': 'ReduceLROnPlateau', 'scheduler_params': {'patience': 5, 'factor': 0.5, 'min_lr': 1e-05}, 'loss': None, 'loss_params': {}, 'loss_on_logits': True, 'clip_grad': False, 'clip_grad_params': {}, 'init_bias': True, 'dataset': 'UniversalDataset', 'tuned': False, 'optimization_search_space': None, 'verbose_bar': False, 'freeze_defaults': False, 'n_out': 1, 'hid_factor': [2, 2], 'hidden_size': [512, 256], 'block_config': [2, 2], 'compression': 0.5, 'growth_size': 256, 'bn_factor': 2, 'drop_rate': 0.1, 'noise_std': 0.05, 'num_init_features': None, 'act_fun': 'LeakyReLU', 'use_noise': False, 'use_bn': True, 'embedding_size': 10, 'cat_embedder': 'cat', 'cont_embedder': 'cont', 'stop_by_metric': False, 'tuning_params': {'fit_on_holdout': True, 'max_tuning_iter': 25, 'max_tuning_time': 3600}, 'device_ids': None, 'num_dims': 58, 'text_features': [], 'bias': array([-2.76907645])}\n",
      "[12:09:40] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m =====\n",
      "[12:10:13] Epoch: 0, train loss: 0.20566728711128235, val loss: 0.19898445904254913, val metric: 0.7812771056443012\n",
      "[12:10:45] Epoch: 1, train loss: 0.19478976726531982, val loss: 0.1938731074333191, val metric: 0.7886870476861895\n",
      "[12:11:18] Epoch: 2, train loss: 0.1927504539489746, val loss: 0.19276800751686096, val metric: 0.7908125178113624\n",
      "[12:11:50] Epoch: 3, train loss: 0.1918364018201828, val loss: 0.1921372264623642, val metric: 0.7923127342649496\n",
      "[12:12:22] Epoch: 4, train loss: 0.19119876623153687, val loss: 0.19171933829784393, val metric: 0.7942681811771239\n",
      "[12:12:54] Epoch: 5, train loss: 0.19073259830474854, val loss: 0.19154295325279236, val metric: 0.7951750995320248\n",
      "[12:13:26] Epoch: 6, train loss: 0.19042155146598816, val loss: 0.19101059436798096, val metric: 0.7968483781290311\n",
      "[12:13:58] Epoch: 7, train loss: 0.19005173444747925, val loss: 0.19196993112564087, val metric: 0.7973990995600383\n",
      "[12:14:30] Epoch: 8, train loss: 0.19005267322063446, val loss: 0.19151729345321655, val metric: 0.7968016761627463\n",
      "[12:15:03] Epoch: 9, train loss: 0.18959589302539825, val loss: 0.1910094916820526, val metric: 0.7982647319686836\n",
      "[12:15:35] Epoch: 10, train loss: 0.18944258987903595, val loss: 0.1903684288263321, val metric: 0.7984719909785877\n",
      "[12:16:07] Epoch: 11, train loss: 0.1893959790468216, val loss: 0.19041630625724792, val metric: 0.7988341576538671\n",
      "[12:16:39] Epoch: 12, train loss: 0.189090296626091, val loss: 0.18988437950611115, val metric: 0.8001058109428147\n",
      "[12:17:11] Epoch: 13, train loss: 0.18900254368782043, val loss: 0.1900399923324585, val metric: 0.8003248182863314\n",
      "[12:17:43] Epoch: 14, train loss: 0.1890053153038025, val loss: 0.19087809324264526, val metric: 0.8005391479182942\n",
      "[12:18:15] Epoch: 15, train loss: 0.1888273060321808, val loss: 0.1904604136943817, val metric: 0.8000839451462749\n",
      "[12:18:48] Epoch: 16, train loss: 0.18868663907051086, val loss: 0.18975254893302917, val metric: 0.8011378264399942\n",
      "[12:19:20] Epoch: 17, train loss: 0.1886833906173706, val loss: 0.19039368629455566, val metric: 0.8012230061435123\n",
      "[12:19:51] Epoch: 18, train loss: 0.18855664134025574, val loss: 0.19027179479599, val metric: 0.801507551548289\n",
      "[12:20:23] Epoch: 19, train loss: 0.18847456574440002, val loss: 0.18950249254703522, val metric: 0.8014121212226365\n",
      "[12:20:56] Epoch: 20, train loss: 0.18845459818840027, val loss: 0.18931086361408234, val metric: 0.8019830261615863\n",
      "[12:21:28] Epoch: 21, train loss: 0.18835589289665222, val loss: 0.18985122442245483, val metric: 0.8021762309423004\n",
      "[12:22:00] Epoch: 22, train loss: 0.18801091611385345, val loss: 0.18991389870643616, val metric: 0.8011715275850742\n",
      "[12:22:32] Epoch: 23, train loss: 0.1882581263780594, val loss: 0.18956860899925232, val metric: 0.8024970576165966\n",
      "[12:23:04] Epoch: 24, train loss: 0.1880539059638977, val loss: 0.1895894706249237, val metric: 0.8021889403953391\n",
      "[12:23:37] Epoch: 25, train loss: 0.1881452351808548, val loss: 0.189353808760643, val metric: 0.8022599524865799\n",
      "[12:24:09] Epoch: 26, train loss: 0.18799757957458496, val loss: 0.18927156925201416, val metric: 0.8029792758867709\n",
      "[12:24:41] Epoch: 27, train loss: 0.18793052434921265, val loss: 0.19058910012245178, val metric: 0.8023124936818593\n",
      "[12:25:13] Epoch: 28, train loss: 0.1878305971622467, val loss: 0.18943940103054047, val metric: 0.8029121082732332\n",
      "[12:25:45] Epoch: 29, train loss: 0.18782919645309448, val loss: 0.1895354688167572, val metric: 0.8026887752330962\n",
      "[12:26:17] Epoch: 30, train loss: 0.18777942657470703, val loss: 0.1897917240858078, val metric: 0.8019055762841227\n",
      "[12:26:49] Epoch: 31, train loss: 0.18778455257415771, val loss: 0.1892530918121338, val metric: 0.8025135311244511\n",
      "[12:27:22] Epoch: 32, train loss: 0.18769928812980652, val loss: 0.1891929656267166, val metric: 0.8025244488610292\n",
      "[12:27:54] Epoch: 33, train loss: 0.18773290514945984, val loss: 0.18911299109458923, val metric: 0.8035472645072432\n",
      "[12:28:26] Epoch: 34, train loss: 0.18765336275100708, val loss: 0.1889583021402359, val metric: 0.8040333602153207\n",
      "[12:28:59] Epoch: 35, train loss: 0.18750092387199402, val loss: 0.1896829903125763, val metric: 0.8025703083646074\n",
      "[12:29:31] Epoch: 36, train loss: 0.18770313262939453, val loss: 0.1896045207977295, val metric: 0.8028644385954826\n",
      "[12:30:04] Epoch: 37, train loss: 0.18744803965091705, val loss: 0.1892589032649994, val metric: 0.8032546333062545\n",
      "[12:30:36] Epoch: 38, train loss: 0.18735380470752716, val loss: 0.18906675279140472, val metric: 0.8035689470450725\n",
      "[12:31:08] Epoch: 39, train loss: 0.18739672005176544, val loss: 0.18945220112800598, val metric: 0.8035077979640164\n",
      "[12:31:40] Epoch: 40, train loss: 0.18744385242462158, val loss: 0.18905939161777496, val metric: 0.803932960809948\n",
      "[12:32:13] Epoch: 41, train loss: 0.18701627850532532, val loss: 0.1887897104024887, val metric: 0.8041106571574573\n",
      "[12:32:45] Epoch: 42, train loss: 0.18687817454338074, val loss: 0.18889757990837097, val metric: 0.8043049232566046\n",
      "[12:33:17] Epoch: 43, train loss: 0.18680816888809204, val loss: 0.1890023946762085, val metric: 0.8047025854302406\n",
      "[12:33:49] Epoch: 44, train loss: 0.18681186437606812, val loss: 0.18915121257305145, val metric: 0.8042086834284453\n",
      "[12:34:21] Epoch: 45, train loss: 0.18681921064853668, val loss: 0.18919521570205688, val metric: 0.8041033439483283\n",
      "[12:34:53] Epoch: 46, train loss: 0.18675774335861206, val loss: 0.18873851001262665, val metric: 0.8043881714924153\n",
      "[12:35:26] Epoch: 47, train loss: 0.18676722049713135, val loss: 0.18890215456485748, val metric: 0.8037641808556855\n",
      "[12:35:58] Epoch: 48, train loss: 0.18669603765010834, val loss: 0.1894974559545517, val metric: 0.8042491110910983\n",
      "[12:36:31] Epoch: 49, train loss: 0.18669353425502777, val loss: 0.18934662640094757, val metric: 0.8041037632020696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:38] Early stopping: val loss: 0.18867024779319763, val metric: 0.8045885141339955\n",
      "[12:36:39] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m =====\n",
      "[12:37:11] Epoch: 0, train loss: 0.2057550996541977, val loss: 0.19623930752277374, val metric: 0.782837355805784\n",
      "[12:37:43] Epoch: 1, train loss: 0.19530603289604187, val loss: 0.19323384761810303, val metric: 0.7896652615900496\n",
      "[12:38:16] Epoch: 2, train loss: 0.19324208796024323, val loss: 0.19249777495861053, val metric: 0.7934528948144519\n",
      "[12:38:48] Epoch: 3, train loss: 0.19227632880210876, val loss: 0.1917622834444046, val metric: 0.7957923109148619\n",
      "[12:39:20] Epoch: 4, train loss: 0.19157063961029053, val loss: 0.1911332756280899, val metric: 0.7960408848759972\n",
      "[12:39:52] Epoch: 5, train loss: 0.19133360683918, val loss: 0.19054950773715973, val metric: 0.7977637171557741\n",
      "[12:40:24] Epoch: 6, train loss: 0.19086650013923645, val loss: 0.19011658430099487, val metric: 0.7987780646674585\n",
      "[12:40:57] Epoch: 7, train loss: 0.1906287670135498, val loss: 0.19150972366333008, val metric: 0.7995717884674286\n",
      "[12:41:29] Epoch: 8, train loss: 0.1905767023563385, val loss: 0.18991024792194366, val metric: 0.7986578838685805\n",
      "[12:42:01] Epoch: 9, train loss: 0.19020618498325348, val loss: 0.1900193989276886, val metric: 0.8008202720543327\n",
      "[12:42:33] Epoch: 10, train loss: 0.1900361031293869, val loss: 0.18973205983638763, val metric: 0.8009416090970194\n",
      "[12:43:05] Epoch: 11, train loss: 0.18994233012199402, val loss: 0.19012956321239471, val metric: 0.8006117803329569\n",
      "[12:43:38] Epoch: 12, train loss: 0.18948982656002045, val loss: 0.18948473036289215, val metric: 0.8009333861863761\n",
      "[12:44:10] Epoch: 13, train loss: 0.189614400267601, val loss: 0.18942014873027802, val metric: 0.8016885201844335\n",
      "[12:44:42] Epoch: 14, train loss: 0.1894819289445877, val loss: 0.19020263850688934, val metric: 0.8018849418806469\n",
      "[12:45:15] Epoch: 15, train loss: 0.18930643796920776, val loss: 0.18965616822242737, val metric: 0.8024565086604085\n",
      "[12:45:47] Epoch: 16, train loss: 0.18930724263191223, val loss: 0.1890748292207718, val metric: 0.8027593654337799\n",
      "[12:46:18] Epoch: 17, train loss: 0.18896640837192535, val loss: 0.18985342979431152, val metric: 0.8029677543193326\n",
      "[12:46:51] Epoch: 18, train loss: 0.1890854686498642, val loss: 0.1890406608581543, val metric: 0.8026438781673519\n",
      "[12:47:23] Epoch: 19, train loss: 0.18899919092655182, val loss: 0.18934793770313263, val metric: 0.8042947820623336\n",
      "[12:47:55] Epoch: 20, train loss: 0.18881025910377502, val loss: 0.188893660902977, val metric: 0.8037070937896513\n",
      "[12:48:27] Epoch: 21, train loss: 0.1887843757867813, val loss: 0.18887899816036224, val metric: 0.803926897451595\n",
      "[12:49:00] Epoch: 22, train loss: 0.18858100473880768, val loss: 0.18861569464206696, val metric: 0.8049165314054778\n",
      "[12:49:32] Epoch: 23, train loss: 0.18861569464206696, val loss: 0.1887708604335785, val metric: 0.8046430078915103\n",
      "[12:50:04] Epoch: 24, train loss: 0.18844902515411377, val loss: 0.1887945979833603, val metric: 0.804167538551845\n",
      "[12:50:37] Epoch: 25, train loss: 0.1883377581834793, val loss: 0.18860286474227905, val metric: 0.8043214640032667\n",
      "[12:51:09] Epoch: 26, train loss: 0.18834874033927917, val loss: 0.1891537308692932, val metric: 0.8036639093358882\n",
      "[12:51:41] Epoch: 27, train loss: 0.18829677999019623, val loss: 0.18851883709430695, val metric: 0.8048710331456892\n",
      "[12:52:13] Epoch: 28, train loss: 0.1881439983844757, val loss: 0.18969465792179108, val metric: 0.804142737979116\n",
      "[12:52:46] Epoch: 29, train loss: 0.18812459707260132, val loss: 0.1883428990840912, val metric: 0.8047221321471234\n",
      "[12:53:18] Epoch: 30, train loss: 0.18802763521671295, val loss: 0.18854868412017822, val metric: 0.8044139213188983\n",
      "[12:53:50] Epoch: 31, train loss: 0.18787075579166412, val loss: 0.1883048713207245, val metric: 0.8052987853449165\n",
      "[12:54:22] Epoch: 32, train loss: 0.1878548562526703, val loss: 0.18853101134300232, val metric: 0.8047419385903805\n",
      "[12:54:54] Epoch: 33, train loss: 0.187717467546463, val loss: 0.18835975229740143, val metric: 0.804687223340323\n",
      "[12:55:27] Epoch: 34, train loss: 0.18767626583576202, val loss: 0.18830731511116028, val metric: 0.8053161856935889\n",
      "[12:55:59] Epoch: 35, train loss: 0.1877916157245636, val loss: 0.18886905908584595, val metric: 0.8046370104535568\n",
      "[12:56:31] Epoch: 36, train loss: 0.18776167929172516, val loss: 0.18865515291690826, val metric: 0.8039671840445928\n",
      "[12:57:03] Epoch: 37, train loss: 0.1877107322216034, val loss: 0.18821987509727478, val metric: 0.8064194044511679\n",
      "[12:57:36] Epoch: 38, train loss: 0.18757933378219604, val loss: 0.18836349248886108, val metric: 0.8054865780608251\n",
      "[12:58:08] Epoch: 39, train loss: 0.18749986588954926, val loss: 0.18864881992340088, val metric: 0.8058187970985764\n",
      "[12:58:40] Epoch: 40, train loss: 0.1873786300420761, val loss: 0.18846508860588074, val metric: 0.8054752793043375\n",
      "[12:59:13] Epoch: 41, train loss: 0.18748614192008972, val loss: 0.1882447451353073, val metric: 0.8062959118114152\n",
      "[12:59:45] Epoch: 42, train loss: 0.1872166395187378, val loss: 0.18847167491912842, val metric: 0.8039923854133513\n",
      "[13:00:17] Epoch: 43, train loss: 0.18738195300102234, val loss: 0.18823757767677307, val metric: 0.8059699894902317\n",
      "[13:00:49] Epoch: 44, train loss: 0.18692956864833832, val loss: 0.1882355660200119, val metric: 0.8064847566169011\n",
      "[13:01:22] Epoch: 45, train loss: 0.18685811758041382, val loss: 0.18806566298007965, val metric: 0.8061794172812817\n",
      "[13:01:54] Epoch: 46, train loss: 0.18701797723770142, val loss: 0.18811222910881042, val metric: 0.8056805448813582\n",
      "[13:02:26] Epoch: 47, train loss: 0.18671321868896484, val loss: 0.18816633522510529, val metric: 0.805838374138843\n",
      "[13:02:58] Epoch: 48, train loss: 0.1868116408586502, val loss: 0.18811362981796265, val metric: 0.8062242919340901\n",
      "[13:03:31] Epoch: 49, train loss: 0.18685953319072723, val loss: 0.188089981675148, val metric: 0.8062540721338033\n",
      "[13:03:38] Early stopping: val loss: 0.18798163533210754, val metric: 0.8062160756154868\n",
      "[13:03:38] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m =====\n",
      "[13:04:11] Epoch: 0, train loss: 0.2054419368505478, val loss: 0.1983042061328888, val metric: 0.7797699820512463\n",
      "[13:04:43] Epoch: 1, train loss: 0.19501760601997375, val loss: 0.1931743323802948, val metric: 0.7863494931778636\n",
      "[13:05:15] Epoch: 2, train loss: 0.1927938312292099, val loss: 0.19325986504554749, val metric: 0.7893537797915974\n",
      "[13:05:47] Epoch: 3, train loss: 0.19182710349559784, val loss: 0.192542165517807, val metric: 0.7904537223053174\n",
      "[13:06:20] Epoch: 4, train loss: 0.19128957390785217, val loss: 0.1945381760597229, val metric: 0.7913316515052999\n",
      "[13:06:52] Epoch: 5, train loss: 0.1908608376979828, val loss: 0.19198672473430634, val metric: 0.7926879360400415\n",
      "[13:07:24] Epoch: 6, train loss: 0.19059662520885468, val loss: 0.19169943034648895, val metric: 0.7934351543165176\n",
      "[13:07:56] Epoch: 7, train loss: 0.1905241310596466, val loss: 0.19147101044654846, val metric: 0.7942535455300097\n",
      "[13:08:28] Epoch: 8, train loss: 0.19026486575603485, val loss: 0.19153431057929993, val metric: 0.7946000191949656\n",
      "[13:09:01] Epoch: 9, train loss: 0.1899784356355667, val loss: 0.19107714295387268, val metric: 0.7956574061355343\n",
      "[13:09:33] Epoch: 10, train loss: 0.1899314671754837, val loss: 0.19132119417190552, val metric: 0.7958997928079652\n",
      "[13:10:05] Epoch: 11, train loss: 0.18982528150081635, val loss: 0.1918189823627472, val metric: 0.7948661385296525\n",
      "[13:10:37] Epoch: 12, train loss: 0.18945690989494324, val loss: 0.19089806079864502, val metric: 0.7963537417739844\n",
      "[13:11:10] Epoch: 13, train loss: 0.1894168108701706, val loss: 0.19075997173786163, val metric: 0.7968313271584764\n",
      "[13:11:42] Epoch: 14, train loss: 0.18913555145263672, val loss: 0.191302090883255, val metric: 0.7976564725761092\n",
      "[13:12:15] Epoch: 15, train loss: 0.18904592096805573, val loss: 0.19063155353069305, val metric: 0.7978976344475159\n",
      "[13:12:47] Epoch: 16, train loss: 0.1890181601047516, val loss: 0.19027508795261383, val metric: 0.7979511064388372\n",
      "[13:13:19] Epoch: 17, train loss: 0.18884052336215973, val loss: 0.19104444980621338, val metric: 0.7975139197120226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:13:51] Epoch: 18, train loss: 0.18879841268062592, val loss: 0.189937561750412, val metric: 0.798740517726267\n",
      "[13:14:24] Epoch: 19, train loss: 0.18872402608394623, val loss: 0.19002273678779602, val metric: 0.7985878091654215\n",
      "[13:14:56] Epoch: 20, train loss: 0.1886286586523056, val loss: 0.19030529260635376, val metric: 0.7985609584682656\n",
      "[13:15:28] Epoch: 21, train loss: 0.18857918679714203, val loss: 0.190310999751091, val metric: 0.798711643272844\n",
      "[13:16:00] Epoch: 22, train loss: 0.18846723437309265, val loss: 0.19084405899047852, val metric: 0.7981742825884232\n",
      "[13:16:33] Epoch: 23, train loss: 0.18835017085075378, val loss: 0.19038909673690796, val metric: 0.7981789339318174\n",
      "[13:17:05] Epoch: 24, train loss: 0.1882804036140442, val loss: 0.19069109857082367, val metric: 0.79844290953511\n",
      "[13:17:37] Epoch: 25, train loss: 0.18800267577171326, val loss: 0.18971383571624756, val metric: 0.7998962368084777\n",
      "[13:18:09] Epoch: 26, train loss: 0.18795877695083618, val loss: 0.18980012834072113, val metric: 0.7999833268851757\n",
      "[13:18:41] Epoch: 27, train loss: 0.18780924379825592, val loss: 0.19072525203227997, val metric: 0.798210516395255\n",
      "[13:19:13] Epoch: 28, train loss: 0.18766973912715912, val loss: 0.18991592526435852, val metric: 0.800207849129321\n",
      "[13:19:46] Epoch: 29, train loss: 0.18782079219818115, val loss: 0.18961867690086365, val metric: 0.8005450227443045\n",
      "[13:20:18] Epoch: 30, train loss: 0.18782956898212433, val loss: 0.1897743046283722, val metric: 0.8000093733534568\n",
      "[13:20:50] Epoch: 31, train loss: 0.18762332201004028, val loss: 0.1905660331249237, val metric: 0.7986917959589392\n",
      "[13:21:22] Epoch: 32, train loss: 0.18761055171489716, val loss: 0.19001492857933044, val metric: 0.7992864520178569\n",
      "[13:21:55] Epoch: 33, train loss: 0.18760672211647034, val loss: 0.19044142961502075, val metric: 0.7994052840853595\n",
      "[13:22:27] Epoch: 34, train loss: 0.18769213557243347, val loss: 0.1895918846130371, val metric: 0.800229696468149\n",
      "[13:22:59] Epoch: 35, train loss: 0.1876358985900879, val loss: 0.189912348985672, val metric: 0.8003011937335309\n",
      "[13:23:32] Epoch: 36, train loss: 0.18754245340824127, val loss: 0.18977560102939606, val metric: 0.800365617740038\n",
      "[13:24:04] Epoch: 37, train loss: 0.1874719262123108, val loss: 0.18959800899028778, val metric: 0.8008836162846951\n",
      "[13:24:36] Epoch: 38, train loss: 0.18721306324005127, val loss: 0.19001273810863495, val metric: 0.7999446856653513\n",
      "[13:25:08] Epoch: 39, train loss: 0.18738244473934174, val loss: 0.1896851360797882, val metric: 0.8001284335055419\n",
      "[13:25:40] Epoch: 40, train loss: 0.1873173713684082, val loss: 0.18951646983623505, val metric: 0.8011722276597176\n",
      "[13:26:13] Epoch: 41, train loss: 0.18730083107948303, val loss: 0.19001437723636627, val metric: 0.8002495951999655\n",
      "[13:26:45] Epoch: 42, train loss: 0.1871936321258545, val loss: 0.18965011835098267, val metric: 0.8004928085142071\n",
      "[13:27:18] Epoch: 43, train loss: 0.18723052740097046, val loss: 0.1896437704563141, val metric: 0.8010118169593855\n",
      "[13:27:50] Epoch: 44, train loss: 0.1871400773525238, val loss: 0.18927660584449768, val metric: 0.801366008584724\n",
      "[13:28:22] Epoch: 45, train loss: 0.18732713162899017, val loss: 0.1895390748977661, val metric: 0.8009955108893461\n",
      "[13:28:55] Epoch: 46, train loss: 0.18705570697784424, val loss: 0.18951015174388885, val metric: 0.8011683673611183\n",
      "[13:29:27] Epoch: 47, train loss: 0.1873127520084381, val loss: 0.19005435705184937, val metric: 0.8005405045601175\n",
      "[13:29:59] Epoch: 48, train loss: 0.18701528012752533, val loss: 0.18990632891654968, val metric: 0.8007900699640623\n",
      "[13:30:32] Epoch: 49, train loss: 0.1871916502714157, val loss: 0.18949487805366516, val metric: 0.801097262981318\n",
      "[13:30:39] Early stopping: val loss: 0.18927738070487976, val metric: 0.8014446964472917\n",
      "[13:30:39] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m =====\n",
      "[13:31:12] Epoch: 0, train loss: 0.20597350597381592, val loss: 0.1968698650598526, val metric: 0.7836501159535842\n",
      "[13:31:45] Epoch: 1, train loss: 0.19518537819385529, val loss: 0.1941547691822052, val metric: 0.790697190931735\n",
      "[13:32:17] Epoch: 2, train loss: 0.19313284754753113, val loss: 0.19246071577072144, val metric: 0.7934001422099071\n",
      "[13:32:49] Epoch: 3, train loss: 0.1922118365764618, val loss: 0.19147059321403503, val metric: 0.7954959119041173\n",
      "[13:33:21] Epoch: 4, train loss: 0.1913863569498062, val loss: 0.19104577600955963, val metric: 0.7965947084979781\n",
      "[13:33:54] Epoch: 5, train loss: 0.1910957396030426, val loss: 0.1906273365020752, val metric: 0.7977552725013496\n",
      "[13:34:26] Epoch: 6, train loss: 0.1908409148454666, val loss: 0.19102716445922852, val metric: 0.7980290479529324\n",
      "[13:34:59] Epoch: 7, train loss: 0.19062776863574982, val loss: 0.19060248136520386, val metric: 0.7983505959659332\n",
      "[13:35:31] Epoch: 8, train loss: 0.19033853709697723, val loss: 0.19018304347991943, val metric: 0.79958987752967\n",
      "[13:36:03] Epoch: 9, train loss: 0.19024701416492462, val loss: 0.18984448909759521, val metric: 0.8005247049101193\n",
      "[13:36:36] Epoch: 10, train loss: 0.18998906016349792, val loss: 0.18988391757011414, val metric: 0.8011439449729691\n",
      "[13:37:08] Epoch: 11, train loss: 0.18987210094928741, val loss: 0.18989814817905426, val metric: 0.801032400088793\n",
      "[13:37:40] Epoch: 12, train loss: 0.18964238464832306, val loss: 0.1895432025194168, val metric: 0.8018296226673125\n",
      "[13:38:12] Epoch: 13, train loss: 0.18954041600227356, val loss: 0.19007451832294464, val metric: 0.8009578821691947\n",
      "[13:38:45] Epoch: 14, train loss: 0.18940746784210205, val loss: 0.19011914730072021, val metric: 0.8017452199058712\n",
      "[13:39:17] Epoch: 15, train loss: 0.1894046515226364, val loss: 0.1895044445991516, val metric: 0.801745670714767\n",
      "[13:39:49] Epoch: 16, train loss: 0.18915382027626038, val loss: 0.18979567289352417, val metric: 0.8025068707625644\n",
      "[13:40:18] Epoch: 17, train loss: 0.18925510346889496, val loss: 0.18936565518379211, val metric: 0.8029781044650122\n",
      "[13:40:40] Epoch: 18, train loss: 0.18904615938663483, val loss: 0.1895078718662262, val metric: 0.8027463412389554\n",
      "[13:41:02] Epoch: 19, train loss: 0.1888633519411087, val loss: 0.1891225427389145, val metric: 0.8034680045556901\n",
      "[13:41:25] Epoch: 20, train loss: 0.1887783259153366, val loss: 0.18920525908470154, val metric: 0.8031513231697176\n",
      "[13:41:47] Epoch: 21, train loss: 0.18869483470916748, val loss: 0.18921419978141785, val metric: 0.8035503404506146\n",
      "[13:42:09] Epoch: 22, train loss: 0.1886727511882782, val loss: 0.18917995691299438, val metric: 0.803037971095494\n",
      "[13:42:31] Epoch: 23, train loss: 0.1885041892528534, val loss: 0.18937119841575623, val metric: 0.8025563041988619\n",
      "[13:42:54] Epoch: 24, train loss: 0.1885124146938324, val loss: 0.18910911679267883, val metric: 0.8044433137840978\n",
      "[13:43:23] Epoch: 25, train loss: 0.18839974701404572, val loss: 0.18903833627700806, val metric: 0.8040858921918523\n",
      "[13:43:49] Epoch: 26, train loss: 0.18841202557086945, val loss: 0.18902504444122314, val metric: 0.8041318667903055\n",
      "[13:44:12] Epoch: 27, train loss: 0.18837524950504303, val loss: 0.1889817863702774, val metric: 0.804254072909423\n",
      "[13:44:34] Epoch: 28, train loss: 0.1883135885000229, val loss: 0.18894726037979126, val metric: 0.8045596290700114\n",
      "[13:44:57] Epoch: 29, train loss: 0.18810589611530304, val loss: 0.18862979114055634, val metric: 0.804848497392524\n",
      "[13:45:19] Epoch: 30, train loss: 0.18828162550926208, val loss: 0.1895909309387207, val metric: 0.8047830088323902\n",
      "[13:45:41] Epoch: 31, train loss: 0.18801850080490112, val loss: 0.1898747980594635, val metric: 0.8036222022885602\n",
      "[13:46:04] Epoch: 32, train loss: 0.18803322315216064, val loss: 0.1885363757610321, val metric: 0.8054671825122592\n",
      "[13:46:26] Epoch: 33, train loss: 0.18793629109859467, val loss: 0.1888277381658554, val metric: 0.8045682432107562\n",
      "[13:46:48] Epoch: 34, train loss: 0.1876986175775528, val loss: 0.18841885030269623, val metric: 0.8058543509008604\n",
      "[13:47:11] Epoch: 35, train loss: 0.18784907460212708, val loss: 0.18853528797626495, val metric: 0.8057661505358255\n",
      "[13:47:33] Epoch: 36, train loss: 0.18773847818374634, val loss: 0.1883961260318756, val metric: 0.8054735623808444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:55] Epoch: 37, train loss: 0.1876065582036972, val loss: 0.18858636915683746, val metric: 0.8051410960927423\n",
      "[13:48:17] Epoch: 38, train loss: 0.18764671683311462, val loss: 0.18856172263622284, val metric: 0.8052514717737294\n",
      "[13:48:39] Epoch: 39, train loss: 0.18763351440429688, val loss: 0.1883734166622162, val metric: 0.8062539283974905\n",
      "[13:49:02] Epoch: 40, train loss: 0.18761155009269714, val loss: 0.18858130276203156, val metric: 0.805409352430737\n",
      "[13:49:24] Epoch: 41, train loss: 0.18743589520454407, val loss: 0.18902719020843506, val metric: 0.8058298147699008\n",
      "[13:49:46] Epoch: 42, train loss: 0.18748611211776733, val loss: 0.18858389556407928, val metric: 0.8055390272141942\n",
      "[13:50:08] Epoch: 43, train loss: 0.18732917308807373, val loss: 0.18835480511188507, val metric: 0.8058904406574705\n",
      "[13:50:30] Epoch: 44, train loss: 0.18742917478084564, val loss: 0.1883849799633026, val metric: 0.8065035394377298\n",
      "[13:50:53] Epoch: 45, train loss: 0.187202587723732, val loss: 0.188429057598114, val metric: 0.8054860426692254\n",
      "[13:51:15] Epoch: 46, train loss: 0.18702247738838196, val loss: 0.1881701797246933, val metric: 0.8066460978648957\n",
      "[13:51:37] Epoch: 47, train loss: 0.18695026636123657, val loss: 0.18831928074359894, val metric: 0.8065127559751568\n",
      "[13:51:59] Epoch: 48, train loss: 0.1869736611843109, val loss: 0.18835166096687317, val metric: 0.8069571560030617\n",
      "[13:52:22] Epoch: 49, train loss: 0.186947762966156, val loss: 0.18853680789470673, val metric: 0.806710020457286\n",
      "[13:52:27] Early stopping: val loss: 0.1880636066198349, val metric: 0.8070493872850664\n",
      "[13:52:27] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m =====\n",
      "[13:52:50] Epoch: 0, train loss: 0.205745667219162, val loss: 0.19651050865650177, val metric: 0.7855267973996979\n",
      "[13:53:12] Epoch: 1, train loss: 0.19532495737075806, val loss: 0.1937745362520218, val metric: 0.7913773067798998\n",
      "[13:53:34] Epoch: 2, train loss: 0.1934252232313156, val loss: 0.1914921998977661, val metric: 0.7948924663213028\n",
      "[13:53:56] Epoch: 3, train loss: 0.19235675036907196, val loss: 0.19168104231357574, val metric: 0.79636402368812\n",
      "[13:54:18] Epoch: 4, train loss: 0.19162845611572266, val loss: 0.19083847105503082, val metric: 0.7968468454060815\n",
      "[13:54:41] Epoch: 5, train loss: 0.19130077958106995, val loss: 0.19061782956123352, val metric: 0.7983754549755944\n",
      "[13:55:03] Epoch: 6, train loss: 0.19119878113269806, val loss: 0.1903238743543625, val metric: 0.7988131931525216\n",
      "[13:55:25] Epoch: 7, train loss: 0.1907167285680771, val loss: 0.1903003454208374, val metric: 0.7989072509066064\n",
      "[13:55:47] Epoch: 8, train loss: 0.19050422310829163, val loss: 0.1901955008506775, val metric: 0.799829019797171\n",
      "[13:56:10] Epoch: 9, train loss: 0.19032703340053558, val loss: 0.189690500497818, val metric: 0.8010488463337412\n",
      "[13:56:32] Epoch: 10, train loss: 0.1901724487543106, val loss: 0.1896151453256607, val metric: 0.8015411765324147\n",
      "[13:56:54] Epoch: 11, train loss: 0.18996870517730713, val loss: 0.18952298164367676, val metric: 0.8015491912376624\n",
      "[13:57:16] Epoch: 12, train loss: 0.18972653150558472, val loss: 0.1895119994878769, val metric: 0.8023192054616481\n",
      "[13:57:38] Epoch: 13, train loss: 0.1896480768918991, val loss: 0.1901123970746994, val metric: 0.8016828742535042\n",
      "[13:58:01] Epoch: 14, train loss: 0.18944047391414642, val loss: 0.1891041398048401, val metric: 0.803460738651215\n",
      "[13:58:23] Epoch: 15, train loss: 0.18932148814201355, val loss: 0.18924929201602936, val metric: 0.8028598664819983\n",
      "[13:58:45] Epoch: 16, train loss: 0.18927069008350372, val loss: 0.18950705230236053, val metric: 0.8017154367128245\n",
      "[13:59:07] Epoch: 17, train loss: 0.1889575719833374, val loss: 0.18929468095302582, val metric: 0.80341640397555\n",
      "[13:59:29] Epoch: 18, train loss: 0.1888815015554428, val loss: 0.18941646814346313, val metric: 0.802405471324301\n",
      "[13:59:52] Epoch: 19, train loss: 0.18881240487098694, val loss: 0.18901076912879944, val metric: 0.8034591130332562\n",
      "[14:00:14] Epoch: 20, train loss: 0.18886007368564606, val loss: 0.18885809183120728, val metric: 0.804468513674718\n",
      "[14:00:36] Epoch: 21, train loss: 0.1886606216430664, val loss: 0.1899743676185608, val metric: 0.8037104549281442\n",
      "[14:00:58] Epoch: 22, train loss: 0.18838787078857422, val loss: 0.18914076685905457, val metric: 0.8031659704754501\n",
      "[14:01:20] Epoch: 23, train loss: 0.1884652078151703, val loss: 0.18919560313224792, val metric: 0.8041610727983707\n",
      "[14:01:43] Epoch: 24, train loss: 0.18830232322216034, val loss: 0.18902024626731873, val metric: 0.8037491427897647\n",
      "[14:02:05] Epoch: 25, train loss: 0.18825829029083252, val loss: 0.18897929787635803, val metric: 0.8037043703970175\n",
      "[14:02:27] Epoch: 26, train loss: 0.18837608397006989, val loss: 0.18892845511436462, val metric: 0.8039942630373313\n",
      "[14:02:49] Epoch: 27, train loss: 0.18792131543159485, val loss: 0.18902568519115448, val metric: 0.8037625524898921\n",
      "[14:03:11] Epoch: 28, train loss: 0.1877410113811493, val loss: 0.18886053562164307, val metric: 0.8045440594241335\n",
      "[14:03:34] Epoch: 29, train loss: 0.18758805096149445, val loss: 0.18859949707984924, val metric: 0.8048589744515008\n",
      "[14:03:56] Epoch: 30, train loss: 0.1875031441450119, val loss: 0.1889764666557312, val metric: 0.8033452235311314\n",
      "[14:04:18] Epoch: 31, train loss: 0.18762627243995667, val loss: 0.18882445991039276, val metric: 0.8044956824575838\n",
      "[14:04:40] Epoch: 32, train loss: 0.18761572241783142, val loss: 0.1888960599899292, val metric: 0.8046415068487549\n",
      "[14:05:02] Epoch: 33, train loss: 0.1875269114971161, val loss: 0.18872258067131042, val metric: 0.8045209896244254\n",
      "[14:05:25] Epoch: 34, train loss: 0.18752992153167725, val loss: 0.18856877088546753, val metric: 0.8050985322642198\n",
      "[14:05:47] Epoch: 35, train loss: 0.1876162737607956, val loss: 0.189083531498909, val metric: 0.8040154171655927\n",
      "[14:06:09] Epoch: 36, train loss: 0.18748393654823303, val loss: 0.18889890611171722, val metric: 0.8041562684578669\n",
      "[14:06:31] Epoch: 37, train loss: 0.18745563924312592, val loss: 0.18922972679138184, val metric: 0.8034042164553477\n",
      "[14:06:54] Epoch: 38, train loss: 0.18741527199745178, val loss: 0.1884845346212387, val metric: 0.8053753223974185\n",
      "[14:07:16] Epoch: 39, train loss: 0.18715938925743103, val loss: 0.18860700726509094, val metric: 0.8052504082258938\n",
      "[14:07:38] Epoch: 40, train loss: 0.1871776282787323, val loss: 0.18864551186561584, val metric: 0.8046814696272981\n",
      "[14:08:00] Epoch: 41, train loss: 0.18720506131649017, val loss: 0.1884627789258957, val metric: 0.8053425779954555\n",
      "[14:08:22] Epoch: 42, train loss: 0.18735119700431824, val loss: 0.18866805732250214, val metric: 0.804711890964506\n",
      "[14:08:45] Epoch: 43, train loss: 0.18715523183345795, val loss: 0.18885649740695953, val metric: 0.8053187355987289\n",
      "[14:09:07] Epoch: 44, train loss: 0.1872446984052658, val loss: 0.18852108716964722, val metric: 0.805565441911506\n",
      "[14:09:29] Epoch: 45, train loss: 0.18718385696411133, val loss: 0.18876968324184418, val metric: 0.8040815625451421\n",
      "[14:09:51] Epoch: 46, train loss: 0.18706250190734863, val loss: 0.18848054111003876, val metric: 0.8060572618797253\n",
      "[14:10:13] Epoch: 47, train loss: 0.18708571791648865, val loss: 0.18857958912849426, val metric: 0.8049701018533635\n",
      "[14:10:36] Epoch: 48, train loss: 0.1869533807039261, val loss: 0.18889474868774414, val metric: 0.8047576482206878\n",
      "[14:10:58] Epoch: 49, train loss: 0.1866934448480606, val loss: 0.1887338012456894, val metric: 0.8045206468339394\n",
      "[14:11:03] Early stopping: val loss: 0.18826685845851898, val metric: 0.8059705543447155\n",
      "[14:11:03] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m finished. score = \u001b[1m0.8050231119875255\u001b[0m\n",
      "[14:11:03] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m fitting and predicting completed\n",
      "[14:11:03] Time left 7091.17 secs\n",
      "\n",
      "[14:11:03] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:11:03] \u001b[1mAutoml preset training completed in 7308.84 seconds\u001b[0m\n",
      "\n",
      "[14:11:03] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0) \n",
      "\n",
      "[2024-11-07 14:11:03,794] - [    END     ] - Fitting TabularLamaNN_fttransformer\n",
      "0.8050231119875255\n"
     ]
    }
   ],
   "source": [
    "# model = TabularLamaNN(n_jobs=N_JOBS, task=\"classification\", nn_name=\"fttransformer\")\n",
    "# model.tune(X_train, y_train, metric, timeout=60 * 60 * 2, categorical_features=cat_columns)\n",
    "# model.verbose = 4\n",
    "# oof = model.fit(X_train, y_train, categorical_features=cat_columns)\n",
    "\n",
    "# print(metric(y_train, oof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, load the fitted model\n",
    "\n",
    "**GPU is required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load(DATA_PATH / \"models\" / \"lamann_fttransformer_8050_full_dataset\" / \"lamann_fttransformer_8050_full_dataset.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model file, parameters, test and oof predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lamann_fttransformer_8050_full_dataset\"\n",
    "MODEL_DIR = DATA_PATH / \"models\" / MODEL_NAME\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res[MODEL_NAME] = oof[:, 1]\n",
    "res.to_csv(MODEL_DIR / \"oof.csv\", index=False)\n",
    "joblib.dump(model, MODEL_DIR / f\"{MODEL_NAME}.joblib\")\n",
    "\n",
    "with (MODEL_DIR / \"params.yaml\").open(\"w\") as f:\n",
    "    yaml.dump(model.params, f)\n",
    "\n",
    "with (MODEL_DIR / \"score.txt\").open(\"w\") as f:\n",
    "    print(\"OOF:\", metric(y_train, oof), file=f)\n",
    "    \n",
    "test = pd.read_parquet(DATA_PATH / \"test_preproc_2.parquet\")\n",
    "test[\"target\"] = model.predict(test[cfg[\"selected_features\"] + cat_columns])[:, 1]\n",
    "test[['id', 'target']].to_csv(MODEL_DIR / f'{MODEL_NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_kernel",
   "language": "python",
   "name": "base_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
