{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa3af6a-d478-4c0e-a715-21f3f6707c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../../../automl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe599f1-83eb-4e6b-8115-f05eb659ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nlp' extra dependecy package 'gensim' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'gensim' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "[2024-11-08 13:26:27]\n",
      "/home/peter/venvs/base_venv/lib/python3.10/site-packages/lightautoml/ml_algo/dl_model.py:42: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n",
      "/home/peter/venvs/base_venv/lib/python3.10/site-packages/lightautoml/text/embed.py:22: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n",
      "/home/peter/venvs/base_venv/lib/python3.10/site-packages/lightautoml/text/dl_transformers.py:25: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.automl.model.lama import TabularLamaUtilized\n",
    "from src.automl.loggers import configure_root_logger\n",
    "from src.automl.constants import create_ml_data_dir\n",
    "from src.automl.model.metrics import RocAuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087a93ef-e784-4dbb-8962-24e871577fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ml_data_dir()\n",
    "configure_root_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4701a7-04a9-42cd-b472-d9ca1dce955d",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9446b478-3af7-4d89-8d52-0d16996734e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 77\n",
    "DATA_PATH = Path(\"../../../data/\")\n",
    "CONFIG_PATH = Path(\"../../../configs/config.yaml\")\n",
    "N_JOBS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5494d0ab-4d12-47b3-bbaa-31fd51961cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with CONFIG_PATH.open() as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1cf46-0712-400d-b190-15de4b596d15",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7208e0bf-13a0-4f7a-9f2e-307f386ef935",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(DATA_PATH / \"train_preproc_2.parquet\")\n",
    "#df_train, df_test = df_train.sort_values(by=\"id\").iloc[:300_000], df_train.sort_values(by=\"id\").iloc[300_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24b73ba9-d6c5-4e83-b7ed-480881d44647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.940982\n",
       "1    0.059018\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83dba0de-f224-4a5a-84db-acb5ff2ba670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersample the 0 class\n",
    "#df_train = pd.concat([df_train.loc[df_train.target == 1], df_train.loc[df_train.target == 0].sample(200_000, random_state=RANDOM_SEED)], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a4bbaca-2b74-4177-88e4-4897ff07498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = df_train.drop(columns=[\"target\", \"id\"]).select_dtypes(int).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2525aeac-d91b-4864-a2cb-24f94702c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train[cfg[\"selected_features\"] + cat_columns], df_train[\"target\"]\n",
    "#X_test, y_test = df_test[cfg[\"selected_features\"] + cat_columns], df_test[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f28cd79-395e-4e5d-9fa6-015a640955d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.940982\n",
       "1    0.059018\n",
       "Name: target, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train.value_counts(normalize=True))\n",
    "#display(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12b00734-0a36-4487-93ed-c78e6833ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical_features = ohe_cols# + oe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abfc9d-95b1-4eda-9665-251676ed3d8f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b32e8180-6799-428d-93d8-50f8c19a074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = RocAuc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c0b86fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-08 13:27:24,414] - [   START    ] - Fitting TabularLamaUtilized\n",
      "[13:27:24] Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n",
      "[13:27:24] - time: 7200.00 seconds\n",
      "[13:27:24] - CPU: 16 cores\n",
      "[13:27:24] - memory: 16 GB\n",
      "\n",
      "[13:27:24] \u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
      "\n",
      "[13:27:24] ==================================================\n",
      "[13:27:24] Start 0 automl preset configuration:\n",
      "[13:27:24] \u001b[1mconf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n",
      "[13:27:24] Stdout logging level is INFO.\n",
      "[13:27:24] Task: binary\n",
      "\n",
      "[13:27:24] Start automl preset with listed constraints:\n",
      "[13:27:24] - time: 7200.00 seconds\n",
      "[13:27:24] - CPU: 16 cores\n",
      "[13:27:24] - memory: 16 GB\n",
      "\n",
      "[13:27:24] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[13:27:35] Layer \u001b[1m1\u001b[0m train process start. Time left 7188.70 secs\n",
      "[13:27:48] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[13:28:33] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7879458135561579\u001b[0m\n",
      "[13:28:33] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[13:28:33] Time left 7131.06 secs\n",
      "\n",
      "[13:28:45] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[13:29:34] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.807851439415669\u001b[0m\n",
      "[13:29:34] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:29:34] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[13:29:34] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[13:34:34] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[13:34:34] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[13:35:37] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8124643025361464\u001b[0m\n",
      "[13:35:37] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:35:37] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[13:36:32] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8069578372829955\u001b[0m\n",
      "[13:36:32] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:36:32] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[13:41:34] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[13:41:34] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[13:43:37] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8078924384298085\u001b[0m\n",
      "[13:43:37] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:43:37] Time left 6227.33 secs\n",
      "\n",
      "[13:43:37] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[13:43:37] Blending: optimization starts with equal weights and score \u001b[1m0.8099334769656599\u001b[0m\n",
      "[13:43:42] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8128415584666869\u001b[0m, weights = \u001b[1m[0.         0.09734569 0.7682273  0.05446735 0.07995959]\u001b[0m\n",
      "[13:43:48] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8128669857951489\u001b[0m, weights = \u001b[1m[0.         0.15655714 0.7124255  0.05992553 0.07109183]\u001b[0m\n",
      "[13:43:54] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8128670833538414\u001b[0m, weights = \u001b[1m[0.         0.15073249 0.7175082  0.06035306 0.07140625]\u001b[0m\n",
      "[13:43:59] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8128671098265244\u001b[0m, weights = \u001b[1m[0.         0.15062104 0.7176024  0.06036098 0.07141562]\u001b[0m\n",
      "[13:44:05] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.8128671098265244\u001b[0m, weights = \u001b[1m[0.         0.15062104 0.7176024  0.06036098 0.07141562]\u001b[0m\n",
      "[13:44:05] Blending: no score update. Terminated\n",
      "\n",
      "[13:44:05] \u001b[1mAutoml preset training completed in 1000.96 seconds\u001b[0m\n",
      "\n",
      "[13:44:05] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.15062 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.71760 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.06036 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.07142 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[13:44:05] ==================================================\n",
      "[13:44:05] Start 1 automl preset configuration:\n",
      "[13:44:05] \u001b[1mconf_1_sel_type_1.yml\u001b[0m, random state: {'reader_params': {'random_state': 43}, 'nn_params': {'random_state': 43}, 'general_params': {'return_all_predictions': False}}\n",
      "[13:44:05] Stdout logging level is INFO.\n",
      "[13:44:05] Task: binary\n",
      "\n",
      "[13:44:05] Start automl preset with listed constraints:\n",
      "[13:44:05] - time: 6199.02 seconds\n",
      "[13:44:05] - CPU: 16 cores\n",
      "[13:44:05] - memory: 16 GB\n",
      "\n",
      "[13:44:05] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[13:44:16] Layer \u001b[1m1\u001b[0m train process start. Time left 6188.11 secs\n",
      "[13:44:29] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[13:45:06] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7876202250782859\u001b[0m\n",
      "[13:45:06] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[13:45:06] Time left 6137.87 secs\n",
      "\n",
      "[13:45:16] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:45:27] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[13:46:18] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8076547164783409\u001b[0m\n",
      "[13:46:18] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:46:18] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[13:51:23] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[13:51:23] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[13:52:31] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8123549376598648\u001b[0m\n",
      "[13:52:31] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:52:31] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[13:53:27] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8070730620089563\u001b[0m\n",
      "[13:53:27] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:53:27] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[13:58:42] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[13:58:42] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:00:30] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8076847717802356\u001b[0m\n",
      "[14:00:30] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:00:30] Time left 5213.60 secs\n",
      "\n",
      "[14:00:30] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:00:31] Blending: optimization starts with equal weights and score \u001b[1m0.8101190890226051\u001b[0m\n",
      "[14:00:36] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8129737483855946\u001b[0m, weights = \u001b[1m[0.         0.10132591 0.69546604 0.08396929 0.11923878]\u001b[0m\n",
      "[14:00:42] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8129891202605231\u001b[0m, weights = \u001b[1m[0.         0.14416976 0.66524607 0.07825273 0.11233138]\u001b[0m\n",
      "[14:00:47] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8129891206823985\u001b[0m, weights = \u001b[1m[0.         0.1441697  0.6652458  0.0782527  0.11233176]\u001b[0m\n",
      "[14:00:53] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8129891200495851\u001b[0m, weights = \u001b[1m[0.         0.1441697  0.66524583 0.0782527  0.11233176]\u001b[0m\n",
      "[14:00:53] Blending: no score update. Terminated\n",
      "\n",
      "[14:00:53] \u001b[1mAutoml preset training completed in 1008.01 seconds\u001b[0m\n",
      "\n",
      "[14:00:53] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.14417 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.66525 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.07825 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.11233 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[14:00:53] ==================================================\n",
      "[14:00:53] Start 2 automl preset configuration:\n",
      "[14:00:53] \u001b[1mconf_2_select_mode_1_no_typ.yml\u001b[0m, random state: {'reader_params': {'random_state': 44}, 'nn_params': {'random_state': 44}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:00:53] Stdout logging level is INFO.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:00:53] Task: binary\n",
      "\n",
      "[14:00:53] Start automl preset with listed constraints:\n",
      "[14:00:53] - time: 5190.98 seconds\n",
      "[14:00:53] - CPU: 16 cores\n",
      "[14:00:53] - memory: 16 GB\n",
      "\n",
      "[14:00:53] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[14:00:54] Layer \u001b[1m1\u001b[0m train process start. Time left 5190.17 secs\n",
      "[14:01:07] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:01:56] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7878750314025083\u001b[0m\n",
      "[14:01:56] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:01:56] Time left 5128.30 secs\n",
      "\n",
      "[14:02:07] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:02:19] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:03:08] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8076809265966576\u001b[0m\n",
      "[14:03:08] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:03:08] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:08:09] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:08:09] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:10:06] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.812175810513778\u001b[0m\n",
      "[14:10:06] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:10:06] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:10:55] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.806652519181102\u001b[0m\n",
      "[14:10:55] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:10:55] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:15:57] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:15:57] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:17:43] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8074051022804657\u001b[0m\n",
      "[14:17:43] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:17:43] Time left 4180.95 secs\n",
      "\n",
      "[14:17:43] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:17:43] Blending: optimization starts with equal weights and score \u001b[1m0.8095102302854604\u001b[0m\n",
      "[14:17:49] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.812476779765568\u001b[0m, weights = \u001b[1m[0.         0.10875404 0.89124596 0.         0.        ]\u001b[0m\n",
      "[14:17:55] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8125672611286026\u001b[0m, weights = \u001b[1m[0.         0.21042496 0.78957504 0.         0.        ]\u001b[0m\n",
      "[14:18:01] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8125672611286026\u001b[0m, weights = \u001b[1m[0.         0.21042496 0.78957504 0.         0.        ]\u001b[0m\n",
      "[14:18:01] Blending: no score update. Terminated\n",
      "\n",
      "[14:18:01] \u001b[1mAutoml preset training completed in 1027.68 seconds\u001b[0m\n",
      "\n",
      "[14:18:01] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.21042 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.78958 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) \n",
      "\n",
      "[14:18:01] ==================================================\n",
      "[14:18:01] Start 3 automl preset configuration:\n",
      "[14:18:01] \u001b[1mconf_3_sel_type_1_no_inter_lgbm.yml\u001b[0m, random state: {'reader_params': {'random_state': 45}, 'nn_params': {'random_state': 45}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:18:01] Stdout logging level is INFO.\n",
      "[14:18:01] Task: binary\n",
      "\n",
      "[14:18:01] Start automl preset with listed constraints:\n",
      "[14:18:01] - time: 4163.27 seconds\n",
      "[14:18:01] - CPU: 16 cores\n",
      "[14:18:01] - memory: 16 GB\n",
      "\n",
      "[14:18:01] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[14:18:12] Layer \u001b[1m1\u001b[0m train process start. Time left 4152.16 secs\n",
      "[14:18:25] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:19:05] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7876062487672695\u001b[0m\n",
      "[14:19:05] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:19:05] Time left 4099.36 secs\n",
      "\n",
      "[14:19:16] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:19:17] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:20:05] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8081465128001653\u001b[0m\n",
      "[14:20:05] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:20:05] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:25:07] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:25:07] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:26:09] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8122458029166284\u001b[0m\n",
      "[14:26:09] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:26:09] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:27:01] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8069951970414997\u001b[0m\n",
      "[14:27:01] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:27:01] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:32:02] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:32:02] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:33:46] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8074204663506988\u001b[0m\n",
      "[14:33:46] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:33:46] Time left 3217.66 secs\n",
      "\n",
      "[14:33:46] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:33:47] Blending: optimization starts with equal weights and score \u001b[1m0.8098117900573794\u001b[0m\n",
      "[14:33:52] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8126887609351313\u001b[0m, weights = \u001b[1m[0.         0.12196411 0.76422    0.05569938 0.05811656]\u001b[0m\n",
      "[14:33:58] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8127327393917386\u001b[0m, weights = \u001b[1m[0.         0.19578423 0.6999689  0.0510165  0.05323046]\u001b[0m\n",
      "[14:34:03] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8127328005109411\u001b[0m, weights = \u001b[1m[0.         0.19544856 0.70026094 0.05103779 0.05325267]\u001b[0m\n",
      "[14:34:09] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8127328005109411\u001b[0m, weights = \u001b[1m[0.         0.19544856 0.70026094 0.05103779 0.05325267]\u001b[0m\n",
      "[14:34:09] Blending: no score update. Terminated\n",
      "\n",
      "[14:34:09] \u001b[1mAutoml preset training completed in 967.79 seconds\u001b[0m\n",
      "\n",
      "[14:34:09] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.19545 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.70026 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.05104 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.05325 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[14:34:09] ==================================================\n",
      "[14:34:09] Start 4 automl preset configuration:\n",
      "[14:34:09] \u001b[1mconf_4_sel_type_0_no_int.yml\u001b[0m, random state: {'reader_params': {'random_state': 46}, 'nn_params': {'random_state': 46}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:34:09] Stdout logging level is INFO.\n",
      "[14:34:09] Task: binary\n",
      "\n",
      "[14:34:09] Start automl preset with listed constraints:\n",
      "[14:34:09] - time: 3195.46 seconds\n",
      "[14:34:09] - CPU: 16 cores\n",
      "[14:34:09] - memory: 16 GB\n",
      "\n",
      "[14:34:09] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[14:34:19] Layer \u001b[1m1\u001b[0m train process start. Time left 3184.81 secs\n",
      "[14:34:21] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:35:11] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7877428157491996\u001b[0m\n",
      "[14:35:11] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:35:11] Time left 3133.00 secs\n",
      "\n",
      "[14:35:12] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:36:02] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8083088889605534\u001b[0m\n",
      "[14:36:02] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:36:02] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:41:03] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:41:03] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:42:01] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8121930536698967\u001b[0m\n",
      "[14:42:01] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:42:02] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:42:54] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8067263183241322\u001b[0m\n",
      "[14:42:54] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:42:54] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:48:05] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:48:05] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:49:53] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8075557040558934\u001b[0m\n",
      "[14:49:53] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:49:53] Time left 2250.78 secs\n",
      "\n",
      "[14:49:53] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:49:53] Blending: optimization starts with equal weights and score \u001b[1m0.8097974977616682\u001b[0m\n",
      "[14:49:59] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8126527270256249\u001b[0m, weights = \u001b[1m[0.         0.13179252 0.75211096 0.         0.11609649]\u001b[0m\n",
      "[14:50:05] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8126923457161023\u001b[0m, weights = \u001b[1m[0.         0.20192623 0.6923848  0.         0.10568899]\u001b[0m\n",
      "[14:50:10] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8126923457161023\u001b[0m, weights = \u001b[1m[0.         0.20192623 0.6923848  0.         0.10568899]\u001b[0m\n",
      "[14:50:10] Blending: no score update. Terminated\n",
      "\n",
      "[14:50:10] \u001b[1mAutoml preset training completed in 961.74 seconds\u001b[0m\n",
      "\n",
      "[14:50:10] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.20193 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.69238 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.10569 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[14:50:10] ==================================================\n",
      "[14:50:10] Start 5 automl preset configuration:\n",
      "[14:50:10] \u001b[1mconf_5_sel_type_1_tuning_full.yml\u001b[0m, random state: {'reader_params': {'random_state': 47}, 'nn_params': {'random_state': 47}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:50:10] Stdout logging level is INFO.\n",
      "[14:50:10] Task: binary\n",
      "\n",
      "[14:50:10] Start automl preset with listed constraints:\n",
      "[14:50:10] - time: 2233.69 seconds\n",
      "[14:50:10] - CPU: 16 cores\n",
      "[14:50:10] - memory: 16 GB\n",
      "\n",
      "[14:50:10] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[14:50:21] Layer \u001b[1m1\u001b[0m train process start. Time left 2222.97 secs\n",
      "[14:50:34] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:51:15] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7876537068523974\u001b[0m\n",
      "[14:51:15] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:51:15] Time left 2168.66 secs\n",
      "\n",
      "[14:51:25] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:51:37] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:52:25] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.807618347019667\u001b[0m\n",
      "[14:52:25] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:52:25] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:52:25] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:53:12] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8072341434336592\u001b[0m\n",
      "[14:53:12] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:53:12] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:54:20] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8108961112072435\u001b[0m\n",
      "[14:54:20] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:54:20] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:55:06] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8076843957310269\u001b[0m\n",
      "[14:55:06] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:55:06] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:55:51] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8087874717221331\u001b[0m\n",
      "[14:55:51] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:55:52] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:56:38] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8059814828087797\u001b[0m\n",
      "[14:56:38] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:56:38] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:57:31] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8109994857688703\u001b[0m\n",
      "[14:57:31] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:57:32] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:57:32] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:58:27] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8109994857688703\u001b[0m\n",
      "[14:58:27] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:58:27] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:59:22] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8064440283978829\u001b[0m\n",
      "[14:59:22] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:59:22] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:59:22] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:00:36] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8072320004646941\u001b[0m\n",
      "[15:00:36] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:00:36] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:01:13] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.804571623331485\u001b[0m\n",
      "[15:01:13] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:01:14] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:02:45] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8067270453736961\u001b[0m\n",
      "[15:02:45] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:02:45] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:03:14] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.7901660634549782\u001b[0m\n",
      "[15:03:14] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:03:14] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:03:33] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.7810105699276635\u001b[0m\n",
      "[15:03:33] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:03:33] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:04:54] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8064033760071408\u001b[0m\n",
      "[15:04:54] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:04:55] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[15:04:55] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:06:48] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8072410041827885\u001b[0m\n",
      "[15:06:48] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:06:48] Time left 1236.34 secs\n",
      "\n",
      "[15:06:48] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[15:06:48] Blending: optimization starts with equal weights and score \u001b[1m0.809089679547442\u001b[0m\n",
      "[15:06:53] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8115833047228669\u001b[0m, weights = \u001b[1m[0.         0.16094835 0.71104413 0.         0.12800752]\u001b[0m\n",
      "[15:06:59] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8116223401178322\u001b[0m, weights = \u001b[1m[0.         0.2333537  0.6496679  0.         0.11697838]\u001b[0m\n",
      "[15:07:04] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8116223404342386\u001b[0m, weights = \u001b[1m[0.         0.2326738  0.6506886  0.         0.11663754]\u001b[0m\n",
      "[15:07:10] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8116223404342386\u001b[0m, weights = \u001b[1m[0.         0.2326738  0.6506886  0.         0.11663754]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:07:10] Blending: no score update. Terminated\n",
      "\n",
      "[15:07:10] \u001b[1mAutoml preset training completed in 1019.65 seconds\u001b[0m\n",
      "\n",
      "[15:07:10] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.23267 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.65069 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.11664 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[15:07:10] ==================================================\n",
      "[15:07:10] Start 6 automl preset configuration:\n",
      "[15:07:10] \u001b[1mconf_6_sel_type_1_tuning_full_no_int_lgbm.yml\u001b[0m, random state: {'reader_params': {'random_state': 48}, 'nn_params': {'random_state': 48}, 'general_params': {'return_all_predictions': False}}\n",
      "[15:07:10] Stdout logging level is INFO.\n",
      "[15:07:10] Task: binary\n",
      "\n",
      "[15:07:10] Start automl preset with listed constraints:\n",
      "[15:07:10] - time: 1214.01 seconds\n",
      "[15:07:10] - CPU: 16 cores\n",
      "[15:07:10] - memory: 16 GB\n",
      "\n",
      "[15:07:10] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[15:07:21] Layer \u001b[1m1\u001b[0m train process start. Time left 1203.45 secs\n",
      "[15:07:33] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[15:08:20] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7876164851526267\u001b[0m\n",
      "[15:08:20] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[15:08:20] Time left 1144.31 secs\n",
      "\n",
      "[15:08:31] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[15:08:31] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[15:09:18] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8075631078114314\u001b[0m\n",
      "[15:09:18] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[15:09:18] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 84.19 secs\n",
      "[15:10:44] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[15:10:44] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[15:11:41] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8112458412954796\u001b[0m\n",
      "[15:11:41] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[15:11:41] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[15:12:39] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8072399829278489\u001b[0m\n",
      "[15:12:39] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:12:39] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[15:17:45] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[15:17:45] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[15:19:12] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8075759614064681\u001b[0m\n",
      "[15:19:12] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[15:19:12] Time left 491.80 secs\n",
      "\n",
      "[15:19:12] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[15:19:12] Blending: optimization starts with equal weights and score \u001b[1m0.8095417395282795\u001b[0m\n",
      "[15:19:18] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8120607766337133\u001b[0m, weights = \u001b[1m[0.         0.13727422 0.6602281  0.11111946 0.09137826]\u001b[0m\n",
      "[15:19:24] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8120843265637914\u001b[0m, weights = \u001b[1m[0.         0.19312277 0.61389506 0.12853554 0.06444657]\u001b[0m\n",
      "[15:19:29] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8120866143414969\u001b[0m, weights = \u001b[1m[0.         0.19033775 0.61427057 0.14336202 0.05202963]\u001b[0m\n",
      "[15:19:35] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8120872337600924\u001b[0m, weights = \u001b[1m[0.         0.18603529 0.6130106  0.14744331 0.05351083]\u001b[0m\n",
      "[15:19:40] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.8120872357112662\u001b[0m, weights = \u001b[1m[0.         0.18603855 0.6130038  0.1474459  0.05351177]\u001b[0m\n",
      "[15:19:40] \u001b[1mAutoml preset training completed in 750.21 seconds\u001b[0m\n",
      "\n",
      "[15:19:40] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.18604 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.61300 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.14745 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.05351 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[15:19:40] ==================================================\n",
      "[15:19:40] Blending: optimization starts with equal weights and score \u001b[1m0.8140943091619701\u001b[0m\n",
      "[15:19:48] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8142515575847478\u001b[0m, weights = \u001b[1m[0.1882052  0.25352237 0.16428886 0.15741904 0.15952097 0.\n",
      " 0.07704351]\u001b[0m\n",
      "[15:19:57] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8142543761872092\u001b[0m, weights = \u001b[1m[0.22066888 0.2560528  0.1515602  0.14921875 0.14579372 0.\n",
      " 0.07670563]\u001b[0m\n",
      "[15:20:05] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8142544259685095\u001b[0m, weights = \u001b[1m[0.22041632 0.25575975 0.15138674 0.15028381 0.14562686 0.\n",
      " 0.07652655]\u001b[0m\n",
      "[15:20:13] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8142544259685095\u001b[0m, weights = \u001b[1m[0.22041632 0.25575975 0.15138674 0.15028381 0.14562686 0.\n",
      " 0.07652655]\u001b[0m\n",
      "[15:20:13] Blending: no score update. Terminated\n",
      "\n",
      "[2024-11-08 15:20:13,135] - [    END     ] - Fitting TabularLamaUtilized\n",
      "0.8142544259685095\n"
     ]
    }
   ],
   "source": [
    "model = TabularLamaUtilized(n_jobs=16, task=\"classification\")\n",
    "model.tune(X_train, y_train, metric, timeout=60 * 60, categorical_features=cat_columns)\n",
    "oof = model.fit(X_train, y_train, categorical_features=cat_columns)\n",
    "\n",
    "print(metric(y_train, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34358089",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lamau_81425_full_dataset\"\n",
    "MODEL_DIR = Path(f\"../../../data/models/{MODEL_NAME}\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "990a471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res[MODEL_NAME] = oof[:, 1]\n",
    "res.to_csv(MODEL_DIR / \"oof.csv\", index=False)\n",
    "joblib.dump(model, MODEL_DIR / f\"{MODEL_NAME}.joblib\")\n",
    "\n",
    "with (MODEL_DIR / \"params.yaml\").open(\"w\") as f:\n",
    "    yaml.dump(model.params, f)\n",
    "\n",
    "with (MODEL_DIR / \"score.txt\").open(\"w\") as f:\n",
    "    print(\"OOF:\", metric(y_train, oof), file=f)\n",
    "    \n",
    "test = pd.read_parquet(DATA_PATH / \"test_preproc_2.parquet\")\n",
    "test[\"target\"] = model.predict(test[cfg[\"selected_features\"] + cat_columns])[:, 1]\n",
    "test[['id', 'target']].to_csv(MODEL_DIR / f'{MODEL_NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d285838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eed1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef917874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a621beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e10759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cd3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bab46f5-c9c4-4567-8fae-11296a05f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-07 12:21:05,781] - [   START    ] - Fitting TabularLamaUtilized\n",
      "[12:21:05] Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n",
      "[12:21:05] - time: 3600.00 seconds\n",
      "[12:21:05] - CPU: 16 cores\n",
      "[12:21:05] - memory: 16 GB\n",
      "\n",
      "[12:21:05] \u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
      "\n",
      "[12:21:05] ==================================================\n",
      "[12:21:05] Start 0 automl preset configuration:\n",
      "[12:21:05] \u001b[1mconf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n",
      "[12:21:05] Stdout logging level is INFO.\n",
      "[12:21:05] Task: binary\n",
      "\n",
      "[12:21:05] Start automl preset with listed constraints:\n",
      "[12:21:05] - time: 3600.00 seconds\n",
      "[12:21:05] - CPU: 16 cores\n",
      "[12:21:05] - memory: 16 GB\n",
      "\n",
      "[12:21:05] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[12:21:18] Layer \u001b[1m1\u001b[0m train process start. Time left 3587.51 secs\n",
      "[12:21:32] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[12:22:23] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7879394460848713\u001b[0m\n",
      "[12:22:23] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[12:22:23] Time left 3522.19 secs\n",
      "\n",
      "[12:22:36] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[12:23:28] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.807851439415669\u001b[0m\n",
      "[12:23:28] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[12:23:28] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[12:23:28] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[12:28:34] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[12:28:34] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[12:29:39] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8124643025361464\u001b[0m\n",
      "[12:29:39] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[12:29:39] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[12:30:38] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8069578372829955\u001b[0m\n",
      "[12:30:38] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[12:30:38] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[12:35:43] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[12:35:43] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[12:37:56] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8078924384298085\u001b[0m\n",
      "[12:37:56] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[12:37:56] Time left 2589.39 secs\n",
      "\n",
      "[12:37:56] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[12:37:56] Blending: optimization starts with equal weights and score \u001b[1m0.8099371512896902\u001b[0m\n",
      "[12:38:02] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8128415584666869\u001b[0m, weights = \u001b[1m[0.         0.09734569 0.7682273  0.05446735 0.07995959]\u001b[0m\n",
      "[12:38:08] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8128669857951489\u001b[0m, weights = \u001b[1m[0.         0.15655714 0.7124255  0.05992553 0.07109183]\u001b[0m\n",
      "[12:38:14] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8128670833538414\u001b[0m, weights = \u001b[1m[0.         0.15073249 0.7175082  0.06035306 0.07140625]\u001b[0m\n",
      "[12:38:20] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8128671098265244\u001b[0m, weights = \u001b[1m[0.         0.15062104 0.7176024  0.06036098 0.07141562]\u001b[0m\n",
      "[12:38:25] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.8128671098265244\u001b[0m, weights = \u001b[1m[0.         0.15062104 0.7176024  0.06036098 0.07141562]\u001b[0m\n",
      "[12:38:25] Blending: no score update. Terminated\n",
      "\n",
      "[12:38:25] \u001b[1mAutoml preset training completed in 1040.00 seconds\u001b[0m\n",
      "\n",
      "[12:38:25] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.15062 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.71760 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.06036 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.07142 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[12:38:25] ==================================================\n",
      "[12:38:25] Start 1 automl preset configuration:\n",
      "[12:38:25] \u001b[1mconf_1_sel_type_1.yml\u001b[0m, random state: {'reader_params': {'random_state': 43}, 'nn_params': {'random_state': 43}, 'general_params': {'return_all_predictions': False}}\n",
      "[12:38:25] Stdout logging level is INFO.\n",
      "[12:38:25] Task: binary\n",
      "\n",
      "[12:38:25] Start automl preset with listed constraints:\n",
      "[12:38:25] - time: 2559.97 seconds\n",
      "[12:38:25] - CPU: 16 cores\n",
      "[12:38:25] - memory: 16 GB\n",
      "\n",
      "[12:38:25] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[12:38:37] Layer \u001b[1m1\u001b[0m train process start. Time left 2548.23 secs\n",
      "[12:38:51] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[12:39:36] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7877210073208837\u001b[0m\n",
      "[12:39:36] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[12:39:36] Time left 2488.98 secs\n",
      "\n",
      "[12:39:47] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[12:39:59] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[12:40:54] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8076547164783409\u001b[0m\n",
      "[12:40:54] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[12:40:54] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[12:46:00] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[12:46:00] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[12:47:13] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.8123549376598648\u001b[0m\n",
      "[12:47:13] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[12:47:13] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[12:48:14] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8070730620089563\u001b[0m\n",
      "[12:48:14] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[12:48:14] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[12:53:20] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[12:53:20] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[12:55:16] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8076847717802356\u001b[0m\n",
      "[12:55:16] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[12:55:16] Time left 1549.11 secs\n",
      "\n",
      "[12:55:16] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[12:55:16] Blending: optimization starts with equal weights and score \u001b[1m0.8101286757198052\u001b[0m\n",
      "[12:55:22] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8129737483855946\u001b[0m, weights = \u001b[1m[0.         0.10132591 0.69546604 0.08396929 0.11923878]\u001b[0m\n",
      "[12:55:28] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8129891202605231\u001b[0m, weights = \u001b[1m[0.         0.14416976 0.66524607 0.07825273 0.11233138]\u001b[0m\n",
      "[12:55:34] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8129891206823985\u001b[0m, weights = \u001b[1m[0.         0.1441697  0.6652458  0.0782527  0.11233176]\u001b[0m\n",
      "[12:55:40] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8129891200495851\u001b[0m, weights = \u001b[1m[0.         0.1441697  0.66524583 0.0782527  0.11233176]\u001b[0m\n",
      "[12:55:40] Blending: no score update. Terminated\n",
      "\n",
      "[12:55:40] \u001b[1mAutoml preset training completed in 1034.63 seconds\u001b[0m\n",
      "\n",
      "[12:55:40] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.14417 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.66525 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.07825 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.11233 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[12:55:40] ==================================================\n",
      "[12:55:40] Start 2 automl preset configuration:\n",
      "[12:55:40] \u001b[1mconf_2_select_mode_1_no_typ.yml\u001b[0m, random state: {'reader_params': {'random_state': 44}, 'nn_params': {'random_state': 44}, 'general_params': {'return_all_predictions': False}}\n",
      "[12:55:40] Stdout logging level is INFO.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:40] Task: binary\n",
      "\n",
      "[12:55:40] Start automl preset with listed constraints:\n",
      "[12:55:40] - time: 1525.31 seconds\n",
      "[12:55:40] - CPU: 16 cores\n",
      "[12:55:40] - memory: 16 GB\n",
      "\n",
      "[12:55:40] \u001b[1mTrain data shape: (413194, 63)\u001b[0m\n",
      "\n",
      "[12:55:41] Layer \u001b[1m1\u001b[0m train process start. Time left 1524.43 secs\n",
      "[12:55:55] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[12:56:43] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7877730815141979\u001b[0m\n",
      "[12:56:43] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[12:56:43] Time left 1462.35 secs\n",
      "\n",
      "[12:56:53] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[12:57:06] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[12:58:00] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8076809265966576\u001b[0m\n",
      "[12:58:00] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[12:58:00] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 135.76 secs\n",
      "[13:00:37] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[13:00:37] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[13:02:39] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.812175810513778\u001b[0m\n",
      "[13:02:39] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:02:39] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[13:03:34] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.806652519181102\u001b[0m\n",
      "[13:03:34] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:03:34] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[13:08:49] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[13:08:49] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[13:10:43] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8074051022804657\u001b[0m\n",
      "[13:10:43] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:10:43] Time left 622.60 secs\n",
      "\n",
      "[13:10:43] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[13:10:43] Blending: optimization starts with equal weights and score \u001b[1m0.8094986425276421\u001b[0m\n",
      "[13:10:49] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.812476779765568\u001b[0m, weights = \u001b[1m[0.         0.10875404 0.89124596 0.         0.        ]\u001b[0m\n",
      "[13:10:55] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8125672611286026\u001b[0m, weights = \u001b[1m[0.         0.21042496 0.78957504 0.         0.        ]\u001b[0m\n",
      "[13:11:01] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8125672611286026\u001b[0m, weights = \u001b[1m[0.         0.21042496 0.78957504 0.         0.        ]\u001b[0m\n",
      "[13:11:01] Blending: no score update. Terminated\n",
      "\n",
      "[13:11:01] \u001b[1mAutoml preset training completed in 920.50 seconds\u001b[0m\n",
      "\n",
      "[13:11:01] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.21042 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.78958 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) \n",
      "\n",
      "[13:11:01] ==================================================\n",
      "[13:11:01] Blending: optimization starts with equal weights and score \u001b[1m0.8140532935364855\u001b[0m\n",
      "[13:11:04] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8140651334176059\u001b[0m, weights = \u001b[1m[0.32514682 0.39961478 0.2752383 ]\u001b[0m\n",
      "[13:11:08] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.814066247801546\u001b[0m, weights = \u001b[1m[0.34526697 0.38603178 0.26870123]\u001b[0m\n",
      "[13:11:11] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8140662230690991\u001b[0m, weights = \u001b[1m[0.3451314  0.38627288 0.2685957 ]\u001b[0m\n",
      "[13:11:11] Blending: no score update. Terminated\n",
      "\n",
      "[2024-11-07 13:11:11,692] - [    END     ] - Fitting TabularLamaUtilized\n",
      "0.814066247801546\n"
     ]
    }
   ],
   "source": [
    "model = TabularLamaUtilized(n_jobs=16, task=\"classification\")\n",
    "model.tune(X_train, y_train, metric, timeout=60 * 30, categorical_features=cat_columns)\n",
    "oof = model.fit(X_train, y_train, categorical_features=cat_columns)\n",
    "\n",
    "print(metric(y_train, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6dd6ac2-6191-43c5-84da-522b4829abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lamau_814_full_dataset\"\n",
    "MODEL_DIR = Path(f\"../../../data/models/{MODEL_NAME}\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e4fed09-b428-4509-8644-347cbbb27f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res[MODEL_NAME] = oof[:, 1]\n",
    "res.to_csv(MODEL_DIR / \"oof.csv\", index=False)\n",
    "joblib.dump(model, MODEL_DIR / f\"{MODEL_NAME}.joblib\")\n",
    "\n",
    "with (MODEL_DIR / \"params.yaml\").open(\"w\") as f:\n",
    "    yaml.dump(model.params, f)\n",
    "\n",
    "with (MODEL_DIR / \"score.txt\").open(\"w\") as f:\n",
    "    print(\"OOF:\", metric(y_train, oof), file=f)\n",
    "    \n",
    "test = pd.read_parquet(DATA_PATH / \"test_preproc_2.parquet\")\n",
    "test[\"target\"] = model.predict(test[cfg[\"selected_features\"] + cat_columns])[:, 1]\n",
    "test[['id', 'target']].to_csv(MODEL_DIR / f'{MODEL_NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f687e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame().assign(names=model.models[0].feature_names_, imp=model.models[0].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d1e1081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>feature_185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          names  imp\n",
       "61  feature_185  0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.sort_values(by=\"imp\", ascending=False).reset_index(drop=True).query(\"names == 'feature_185'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ce7e25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_7',\n",
       " 'feature_31',\n",
       " 'feature_60',\n",
       " 'feature_61',\n",
       " 'feature_71',\n",
       " 'feature_109',\n",
       " 'feature_122',\n",
       " 'feature_156',\n",
       " 'feature_163',\n",
       " 'feature_167',\n",
       " 'feature_179',\n",
       " 'feature_185']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517948d",
   "metadata": {},
   "source": [
    "## With Time series cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fcb3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = RocAuc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d859f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sort_values(by=\"id\").reset_index(drop=True)\n",
    "X_train, y_train = df_train[cfg[\"selected_features\"] + cat_columns], df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52d68ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-07 08:27:33,388] - [   START    ] - Tuning LightGBMClassification\n",
      "[2024-11-07 08:27:40,118] - [   OPTUNA   ] - Trial 0. New best score 0.7903405446081995 with parameters {'max_depth': 6, 'num_leaves': 488, 'min_data_in_leaf': 188, 'bagging_fraction': 0.7993292420985183, 'bagging_freq': 0, 'feature_fraction': 0.49359671220172163, 'lambda_l1': 0.5808361216819946, 'lambda_l2': 8.661761457749352, 'min_gain_to_split': 12.022300234864176, 'is_unbalance': True, 'num_iterations': 2}\n",
      "[2024-11-07 08:28:03,043] - [   OPTUNA   ] - Trial 2. New best score 0.7913515589848906 with parameters {'max_depth': 5, 'num_leaves': 194, 'min_data_in_leaf': 117, 'bagging_fraction': 0.8925879806965068, 'bagging_freq': 0, 'feature_fraction': 0.708540663048167, 'lambda_l1': 5.924145688620425, 'lambda_l2': 0.46450412719997725, 'min_gain_to_split': 12.150897038028766, 'is_unbalance': True, 'num_iterations': 2}\n",
      "[2024-11-07 08:28:12,225] - [   OPTUNA   ] - Trial 3. New best score 0.8048686053278628 with parameters {'max_depth': 16, 'num_leaves': 495, 'min_data_in_leaf': 207, 'bagging_fraction': 0.6523068845866853, 'bagging_freq': 0, 'feature_fraction': 0.8105398159072941, 'lambda_l1': 4.4015249373960135, 'lambda_l2': 1.2203823484477883, 'min_gain_to_split': 9.903538202225404, 'is_unbalance': False, 'num_iterations': 201}\n",
      "[2024-11-07 08:28:27,946] - [   OPTUNA   ] - Trial 5. New best score 0.805775359050857 with parameters {'max_depth': 15, 'num_leaves': 54, 'min_data_in_leaf': 51, 'bagging_fraction': 0.522613644455269, 'bagging_freq': 0, 'feature_fraction': 0.6332063738136893, 'lambda_l1': 2.713490317738959, 'lambda_l2': 8.287375091519294, 'min_gain_to_split': 7.135066533871786, 'is_unbalance': False, 'num_iterations': 219}\n",
      "[2024-11-07 08:30:01,873] - [   OPTUNA   ] - Trial 13. New best score 0.8069467403107815 with parameters {'max_depth': 12, 'num_leaves': 120, 'min_data_in_leaf': 83, 'bagging_fraction': 0.5973813098180023, 'bagging_freq': 0, 'feature_fraction': 0.6464487752219038, 'lambda_l1': 1.815539729792131, 'lambda_l2': 9.653379460654552, 'min_gain_to_split': 5.109079440883124, 'is_unbalance': False, 'num_iterations': 194}\n",
      "[2024-11-07 08:30:23,656] - [   OPTUNA   ] - Trial 15. New best score 0.8079174231410097 with parameters {'max_depth': 9, 'num_leaves': 123, 'min_data_in_leaf': 79, 'bagging_fraction': 0.6696558155271586, 'bagging_freq': 0, 'feature_fraction': 0.41254813989004224, 'lambda_l1': 1.5400879612744933, 'lambda_l2': 6.428485487443978, 'min_gain_to_split': 3.4073761639683475, 'is_unbalance': False, 'num_iterations': 221}\n",
      "[2024-11-07 08:30:47,866] - [   OPTUNA   ] - Trial 17. New best score 0.8080338307668665 with parameters {'max_depth': 9, 'num_leaves': 177, 'min_data_in_leaf': 149, 'bagging_fraction': 0.7020257462335121, 'bagging_freq': 0, 'feature_fraction': 0.4001314682445803, 'lambda_l1': 4.277484898844048, 'lambda_l2': 5.915176709957256, 'min_gain_to_split': 2.630193834916201, 'is_unbalance': False, 'num_iterations': 247}\n",
      "[2024-11-07 08:32:56,360] - [   OPTUNA   ] - Trial 26. New best score 0.8083158418056456 with parameters {'max_depth': 13, 'num_leaves': 120, 'min_data_in_leaf': 166, 'bagging_fraction': 0.8112271899455036, 'bagging_freq': 20, 'feature_fraction': 0.5385502023308056, 'lambda_l1': 1.3349604628055485, 'lambda_l2': 8.855392863089838, 'min_gain_to_split': 0.42505008659048693, 'is_unbalance': False, 'num_iterations': 206}\n",
      "[2024-11-07 08:33:39,494] - [   OPTUNA   ] - Trial 28. New best score 0.8085529083857821 with parameters {'max_depth': 14, 'num_leaves': 75, 'min_data_in_leaf': 170, 'bagging_fraction': 0.8264267643219445, 'bagging_freq': 20, 'feature_fraction': 0.5742524097970673, 'lambda_l1': 1.2310934175206736, 'lambda_l2': 8.969396792235317, 'min_gain_to_split': 0.16644360284870074, 'is_unbalance': False, 'num_iterations': 211}\n",
      "[2024-11-07 08:35:41,908] - [   OPTUNA   ] - Trial 35. New best score 0.8085624905981069 with parameters {'max_depth': 13, 'num_leaves': 45, 'min_data_in_leaf': 179, 'bagging_fraction': 0.9090867052918509, 'bagging_freq': 20, 'feature_fraction': 0.5986896700241908, 'lambda_l1': 1.0457599526213117, 'lambda_l2': 8.511810626751942, 'min_gain_to_split': 1.6394745612739139, 'is_unbalance': False, 'num_iterations': 242}\n",
      "[2024-11-07 08:36:11,789] - [   OPTUNA   ] - Trial 37. New best score 0.8086060147129036 with parameters {'max_depth': 15, 'num_leaves': 146, 'min_data_in_leaf': 131, 'bagging_fraction': 0.9727869256250319, 'bagging_freq': 20, 'feature_fraction': 0.594412726250827, 'lambda_l1': 1.3076349982497943, 'lambda_l2': 8.42304491740083, 'min_gain_to_split': 1.95638379105971, 'is_unbalance': False, 'num_iterations': 180}\n",
      "[2024-11-07 08:37:31,532] - [   OPTUNA   ] - Trial 42. New best score 0.8086748450733487 with parameters {'max_depth': 11, 'num_leaves': 95, 'min_data_in_leaf': 155, 'bagging_fraction': 0.9696680239166591, 'bagging_freq': 20, 'feature_fraction': 0.5068877996580057, 'lambda_l1': 0.3880584908457285, 'lambda_l2': 9.519643874182023, 'min_gain_to_split': 1.5176677728376484, 'is_unbalance': False, 'num_iterations': 225}\n",
      "[2024-11-07 08:43:11,113] - [   OPTUNA   ] - Trial 63. New best score 0.8087325744854885 with parameters {'max_depth': 16, 'num_leaves': 160, 'min_data_in_leaf': 166, 'bagging_fraction': 0.936885833403046, 'bagging_freq': 20, 'feature_fraction': 0.4498188675696397, 'lambda_l1': 1.1357746192947442, 'lambda_l2': 7.7534455607773385, 'min_gain_to_split': 2.069412187433082, 'is_unbalance': False, 'num_iterations': 207}\n",
      "[2024-11-07 08:45:27,863] - [   OPTUNA   ] - Trial 71. New best score 0.809046586456055 with parameters {'max_depth': 10, 'num_leaves': 153, 'min_data_in_leaf': 160, 'bagging_fraction': 0.9135728904594532, 'bagging_freq': 10, 'feature_fraction': 0.506921563666065, 'lambda_l1': 0.6725410135999423, 'lambda_l2': 8.841591608898721, 'min_gain_to_split': 1.6964405734320749, 'is_unbalance': False, 'num_iterations': 237}\n",
      "[2024-11-07 08:51:41,348] - [   OPTUNA   ] - Trial 91. New best score 0.8092009955871362 with parameters {'max_depth': 8, 'num_leaves': 331, 'min_data_in_leaf': 57, 'bagging_fraction': 0.9120914616559199, 'bagging_freq': 10, 'feature_fraction': 0.43368773726287124, 'lambda_l1': 3.8666345262680837, 'lambda_l2': 8.173281721666852, 'min_gain_to_split': 0.7062609644931823, 'is_unbalance': False, 'num_iterations': 313}\n",
      "[2024-11-07 09:24:15,316] - [   OPTUNA   ] - Trial 184. New best score 0.8092209952891476 with parameters {'max_depth': 8, 'num_leaves': 258, 'min_data_in_leaf': 21, 'bagging_fraction': 0.8833000021442096, 'bagging_freq': 10, 'feature_fraction': 0.44326798567844145, 'lambda_l1': 1.7734419425628303, 'lambda_l2': 8.481446649565262, 'min_gain_to_split': 0.19263364585511303, 'is_unbalance': False, 'num_iterations': 294}\n",
      "[2024-11-07 09:27:54,872] - [   OPTUNA   ] - 195 trials completed\n",
      "[2024-11-07 09:27:54,874] - [BEST PARAMS ] - {'objective_type': 'binary', 'boosting': 'gbdt', 'num_iterations': 294, 'max_depth': 8, 'learning_rate': 0.03, 'num_leaves': 258, 'min_data_in_leaf': 21, 'bagging_fraction': 0.8833000021442096, 'bagging_freq': 10, 'feature_fraction': 0.44326798567844145, 'early_stopping_round': 100, 'lambda_l1': 1.7734419425628303, 'lambda_l2': 8.481446649565262, 'min_gain_to_split': 0.19263364585511303, 'num_threads': 16, 'random_state': 42, 'is_unbalance': False, 'num_classes': 1, 'verbose': -1}\n",
      "[2024-11-07 09:27:54,875] - [    END     ] - Tuning LightGBMClassification\n",
      "[2024-11-07 09:27:54,877] - [   START    ] - Fitting LightGBMClassification\n",
      "[2024-11-07 09:27:54,892] - [    FIT     ] - LightGBMClassification fold 0\n",
      "[2024-11-07 09:27:56,881] - [    FIT     ] - LightGBMClassification fold 1\n",
      "[2024-11-07 09:27:59,788] - [    FIT     ] - LightGBMClassification fold 2\n",
      "[2024-11-07 09:28:04,156] - [    FIT     ] - LightGBMClassification fold 3\n",
      "[2024-11-07 09:28:09,145] - [    FIT     ] - LightGBMClassification fold 4\n",
      "[2024-11-07 09:28:14,959] - [    END     ] - Fitting LightGBMClassification\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = LightGBMClassification(n_jobs=16, time_series=True)\n",
    "model.tune(X_train, y_train, metric, timeout=60 * 60, categorical_features=cat_columns)\n",
    "oof = model.fit(X_train, y_train, categorical_features=cat_columns)\n",
    "\n",
    "print(metric(y_train, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a073f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "none_oofs_idx = oof[np.any(np.isnan(oof), axis=1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f8165e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8095227594190041"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y_train[none_oofs_idx:], oof[none_oofs_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc0326db",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lgb_8095_full_dataset_time_series\"\n",
    "MODEL_DIR = Path(f\"../../../data/models/{MODEL_NAME}\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "652cae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res[MODEL_NAME] = oof[none_oofs_idx:, 1]\n",
    "res.to_csv(MODEL_DIR / \"oof.csv\", index=False)\n",
    "joblib.dump(model, MODEL_DIR / f\"{MODEL_NAME}.joblib\")\n",
    "\n",
    "with (MODEL_DIR / \"params.yaml\").open(\"w\") as f:\n",
    "    yaml.dump(model.params, f)\n",
    "\n",
    "with (MODEL_DIR / \"score.txt\").open(\"w\") as f:\n",
    "    print(\"OOF:\", metric(y_train, oof), file=f)\n",
    "    \n",
    "test = pd.read_parquet(DATA_PATH / \"test_preproc_2.parquet\")\n",
    "test[\"target\"] = model.predict(test[cfg[\"selected_features\"] + cat_columns])[:, 1]\n",
    "test[['id', 'target']].to_csv(MODEL_DIR / f'{MODEL_NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c20b42",
   "metadata": {},
   "source": [
    "## TEST \n",
    "**81.22112399468679**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e490c4a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3645213-67f1-4a56-8449-706616b01975",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(DATA_PATH / \"test_preproc_2.parquet\")\n",
    "test[\"target\"] = model.predict(test[cfg[\"selected_features\"] + cat_columns])[:, 1]\n",
    "test[['id', 'target']].to_csv('lgb_813.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8b626f4-24d7-47cc-8e1c-8d4829422eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = pd.read_csv(\"lama_utilized.csv\")\n",
    "pred_2 = pd.read_csv(\"lgmb_oe_ohe_cols_0805.csv\")\n",
    "pred_3 = pd.read_csv(\"catboost_ts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6fd1b74-1c84-47a2-9640-f9afca52a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1[\"target\"] = 0.6 * pred_1[\"target\"] + 0.2 * pred_2[\"target\"] + 0.2 * pred_3[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9a12186-27a7-43e5-bbf9-05d9894bd617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1.to_csv(\"blend.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1a479dff-a006-4237-9a22-9b4a9d6a52eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Open the file pointed by this path and return a file object, as\n",
       "the built-in open() function does.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/lib/python3.10/pathlib.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_DIR.open?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9b1c6-0fb4-461a-9aa9-1c23e299671f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_kernel",
   "language": "python",
   "name": "base_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
