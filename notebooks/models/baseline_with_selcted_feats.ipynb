{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa3af6a-d478-4c0e-a715-21f3f6707c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../../automl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe599f1-83eb-4e6b-8115-f05eb659ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nlp' extra dependecy package 'gensim' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'gensim' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "[2024-11-05 15:30:39]\n",
      "/home/peter/venvs/base_venv/lib/python3.10/site-packages/lightautoml/ml_algo/dl_model.py:42: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n",
      "/home/peter/venvs/base_venv/lib/python3.10/site-packages/lightautoml/text/embed.py:22: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n",
      "/home/peter/venvs/base_venv/lib/python3.10/site-packages/lightautoml/text/dl_transformers.py:25: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.automl.model import AutoML\n",
    "from src.automl.model.metrics import RocAuc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4701a7-04a9-42cd-b472-d9ca1dce955d",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9446b478-3af7-4d89-8d52-0d16996734e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 77\n",
    "DATA_PATH = Path(\"../../data/\")\n",
    "CONFIG_PATH = Path(\"../../configs/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5494d0ab-4d12-47b3-bbaa-31fd51961cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with CONFIG_PATH.open() as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1cf46-0712-400d-b190-15de4b596d15",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7208e0bf-13a0-4f7a-9f2e-307f386ef935",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(DATA_PATH / \"train_preproc.parquet\")\n",
    "df_train, df_test = df_train.sort_values(by=\"id\").iloc[:300_000], df_train.sort_values(by=\"id\").iloc[300_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4bbaca-2b74-4177-88e4-4897ff07498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = df_train.columns[df_train.columns.str.startswith(\"OneHotEncoder\")].values.tolist()\n",
    "oe_cols = df_train.columns[df_train.columns.str.startswith(\"OrdinalEncoder\")].values.tolist()\n",
    "te_cols = df_train.columns[df_train.columns.str.startswith(\"MeanTargetEncoder\")].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979b253b-f611-498c-919d-11993f85a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ordinal encoded columns\n",
    "X_train, y_train = df_train[cfg[\"selected_features\"] + oe_cols + ohe_cols], df_train[\"target\"]\n",
    "X_test, y_test = df_test[cfg[\"selected_features\"] + oe_cols + ohe_cols], df_test[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f28cd79-395e-4e5d-9fa6-015a640955d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.940853\n",
       "1    0.059147\n",
       "Name: target, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0.941322\n",
       "1    0.058678\n",
       "Name: target, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train.value_counts(normalize=True))\n",
    "display(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b00734-0a36-4487-93ed-c78e6833ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ohe_cols + oe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abfc9d-95b1-4eda-9665-251676ed3d8f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e9b58f-d170-4930-9bd9-e67ac8828af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoML(\"classification\", RocAuc(), n_jobs=16, random_state=RANDOM_SEED, tuning_timeout=60 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f4479c2-feee-44cc-9434-196bee461a26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 12:57:58,736] - [   MODEL    ] - 1 out of 15. LogisticRegression\n",
      "[2024-11-05 12:57:58,739] - [   START    ] - Working with LogisticRegression\n",
      "[2024-11-05 12:57:58,740] - [   START    ] - Tuning LogisticRegression\n",
      "[2024-11-05 12:58:18,447] - [BEST PARAMS ] - {'C': 0.005994842503189409, 'class_weight': 'balanced', 'max_iter': 1000, 'n_jobs': 16, 'random_state': 77}\n",
      "[2024-11-05 12:58:18,448] - [    END     ] - Tuning LogisticRegression\n",
      "[2024-11-05 12:58:18,449] - [   START    ] - Fitting LogisticRegression\n",
      "[2024-11-05 12:58:18,526] - [    FIT     ] - LogisticRegression fold 0\n",
      "[2024-11-05 12:58:21,700] - [    FIT     ] - LogisticRegression fold 1\n",
      "[2024-11-05 12:58:24,292] - [    FIT     ] - LogisticRegression fold 2\n",
      "[2024-11-05 12:58:27,107] - [    FIT     ] - LogisticRegression fold 3\n",
      "[2024-11-05 12:58:29,872] - [    FIT     ] - LogisticRegression fold 4\n",
      "[2024-11-05 12:58:32,666] - [    END     ] - Fitting LogisticRegression\n",
      "[2024-11-05 12:58:33,210] - [   SCORE    ] - Train: 0.7873801349058307\n",
      "[2024-11-05 12:58:33,321] - [   SCORE    ] - OOF: 0.7863157048028818\n",
      "[2024-11-05 12:58:33,538] - [   SCORE    ] - Test: 0.7818166173283976\n",
      "[2024-11-05 12:58:33,539] - [   SCORE    ] - Overfit: 0.71 %\n",
      "[2024-11-05 12:58:34,247] - [    END     ] - Working with LogisticRegression\n",
      "[2024-11-05 12:58:34,248] - [  NEW BEST  ] - LogisticRegression. Best score: 0.7818166173283976 \n",
      "\n",
      "[2024-11-05 12:58:34,249] - [   MODEL    ] - 2 out of 15. RandomForestClassification\n",
      "[2024-11-05 12:58:34,250] - [   START    ] - Working with RandomForestClassification\n",
      "[2024-11-05 12:58:34,251] - [   START    ] - Tuning RandomForestClassification\n",
      "[2024-11-05 13:02:05,781] - [   OPTUNA   ] - Trial 0. New best score 0.7851751276510461 with parameters {'n_estimators': 921, 'max_depth': 10, 'min_samples_split': 0.15074244594583536, 'min_samples_leaf': 0.027862913620547006, 'max_features': 0.17858759319064005, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'oob_score': False, 'max_samples': 0.8383131416703828}\n",
      "[2024-11-05 13:05:47,726] - [   OPTUNA   ] - 2 trials completed\n",
      "[2024-11-05 13:05:47,727] - [BEST PARAMS ] - {'n_estimators': 921, 'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 0.15074244594583536, 'min_samples_leaf': 0.027862913620547006, 'max_features': 0.17858759319064005, 'bootstrap': True, 'oob_score': False, 'max_samples': 0.8383131416703828, 'n_jobs': 16, 'random_state': 77, 'class_weight': 'balanced_subsample'}\n",
      "[2024-11-05 13:05:47,728] - [    END     ] - Tuning RandomForestClassification\n",
      "[2024-11-05 13:05:47,730] - [   START    ] - Fitting RandomForestClassification\n",
      "[2024-11-05 13:05:47,804] - [    FIT     ] - RandomForestClassification fold 0\n",
      "[2024-11-05 13:06:31,423] - [    FIT     ] - RandomForestClassification fold 1\n",
      "[2024-11-05 13:07:14,851] - [    FIT     ] - RandomForestClassification fold 2\n",
      "[2024-11-05 13:07:57,822] - [    FIT     ] - RandomForestClassification fold 3\n",
      "[2024-11-05 13:08:41,328] - [    FIT     ] - RandomForestClassification fold 4\n",
      "[2024-11-05 13:09:24,706] - [    END     ] - Fitting RandomForestClassification\n",
      "[2024-11-05 13:09:28,443] - [   SCORE    ] - Train: 0.7864174784314775\n",
      "[2024-11-05 13:09:28,550] - [   SCORE    ] - OOF: 0.7850345866890197\n",
      "[2024-11-05 13:09:30,464] - [   SCORE    ] - Test: 0.7798273847016527\n",
      "[2024-11-05 13:09:30,465] - [   SCORE    ] - Overfit: 0.84 %\n",
      "[2024-11-05 13:09:31,108] - [    END     ] - Working with RandomForestClassification\n",
      "[2024-11-05 13:09:31,109] - [BEST  MODEL ] - LogisticRegression. Best score: 0.7818166173283976 \n",
      "\n",
      "[2024-11-05 13:09:31,110] - [   MODEL    ] - 3 out of 15. ExtraTreesClassification\n",
      "[2024-11-05 13:09:31,111] - [   START    ] - Working with ExtraTreesClassification\n",
      "[2024-11-05 13:09:31,112] - [   START    ] - Tuning ExtraTreesClassification\n",
      "[2024-11-05 13:10:36,148] - [   OPTUNA   ] - Trial 0. New best score 0.7827774923067098 with parameters {'n_estimators': 921, 'max_depth': 10, 'min_samples_split': 0.15074244594583536, 'min_samples_leaf': 0.027862913620547006, 'max_features': 0.17858759319064005, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'oob_score': False, 'max_samples': 0.8383131416703828}\n",
      "[2024-11-05 13:12:19,555] - [   OPTUNA   ] - Trial 2. New best score 0.7831213418239296 with parameters {'n_estimators': 82, 'max_depth': 8, 'min_samples_split': 0.07283448268860333, 'min_samples_leaf': 0.03039430513851218, 'max_features': 0.5920393057443242, 'criterion': 'log_loss', 'class_weight': 'balanced', 'oob_score': True, 'max_samples': 0.35971264713593165}\n",
      "[2024-11-05 13:14:37,752] - [   OPTUNA   ] - 7 trials completed\n",
      "[2024-11-05 13:14:37,754] - [BEST PARAMS ] - {'n_estimators': 82, 'criterion': 'log_loss', 'max_depth': 8, 'min_samples_split': 0.07283448268860333, 'min_samples_leaf': 0.03039430513851218, 'max_features': 0.5920393057443242, 'bootstrap': True, 'oob_score': True, 'max_samples': 0.35971264713593165, 'n_jobs': 16, 'random_state': 77, 'class_weight': 'balanced', 'verbose': 0}\n",
      "[2024-11-05 13:14:37,754] - [    END     ] - Tuning ExtraTreesClassification\n",
      "[2024-11-05 13:14:37,756] - [   START    ] - Fitting ExtraTreesClassification\n",
      "[2024-11-05 13:14:37,848] - [    FIT     ] - ExtraTreesClassification fold 0\n",
      "[2024-11-05 13:14:41,509] - [    FIT     ] - ExtraTreesClassification fold 1\n",
      "[2024-11-05 13:14:45,170] - [    FIT     ] - ExtraTreesClassification fold 2\n",
      "[2024-11-05 13:14:48,824] - [    FIT     ] - ExtraTreesClassification fold 3\n",
      "[2024-11-05 13:14:52,465] - [    FIT     ] - ExtraTreesClassification fold 4\n",
      "[2024-11-05 13:14:56,115] - [    END     ] - Fitting ExtraTreesClassification\n",
      "[2024-11-05 13:14:56,701] - [   SCORE    ] - Train: 0.7834449249716082\n",
      "[2024-11-05 13:14:56,815] - [   SCORE    ] - OOF: 0.7830998602616961\n",
      "[2024-11-05 13:14:57,132] - [   SCORE    ] - Test: 0.7786923753276416\n",
      "[2024-11-05 13:14:57,133] - [   SCORE    ] - Overfit: 0.61 %\n",
      "[2024-11-05 13:14:57,777] - [    END     ] - Working with ExtraTreesClassification\n",
      "[2024-11-05 13:14:57,778] - [BEST  MODEL ] - LogisticRegression. Best score: 0.7818166173283976 \n",
      "\n",
      "[2024-11-05 13:14:57,779] - [   MODEL    ] - 4 out of 15. CatBoostClassification\n",
      "[2024-11-05 13:14:57,780] - [   START    ] - Working with CatBoostClassification\n",
      "[2024-11-05 13:14:57,781] - [   START    ] - Tuning CatBoostClassification\n",
      "[2024-11-05 13:17:20,331] - [   OPTUNA   ] - Trial 0. New best score 0.7993246319642113 with parameters {'boosting_type': 'Plain', 'depth': 15, 'l2_leaf_reg': 128.43911998477108, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Depthwise', 'min_data_in_leaf': 139, 'rsm': 0.5441411055208036, 'subsample': 0.7272537553455429, 'model_size_reg': 80.11090073597784, 'auto_class_weights': 'Balanced', 'iterations': 96}\n",
      "[2024-11-05 13:18:38,485] - [   OPTUNA   ] - Trial 1. New best score 0.8013202125215256 with parameters {'boosting_type': 'Plain', 'depth': 5, 'l2_leaf_reg': 56.20353813079284, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Depthwise', 'min_data_in_leaf': 45, 'rsm': 0.42962619958869225, 'subsample': 0.5754852032169728, 'model_size_reg': 13.359826218292259, 'auto_class_weights': None, 'iterations': 917}\n",
      "[2024-11-05 13:20:05,230] - [   OPTUNA   ] - Trial 2. New best score 0.8025709024597971 with parameters {'boosting_type': 'Plain', 'depth': 6, 'l2_leaf_reg': 30.39430513851218, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Depthwise', 'min_data_in_leaf': 44, 'rsm': 0.7866218528265505, 'subsample': 0.46452464728155557, 'model_size_reg': 70.64901962342054, 'auto_class_weights': 'SqrtBalanced', 'iterations': 683}\n",
      "[2024-11-05 13:20:05,232] - [   OPTUNA   ] - 3 trials completed\n",
      "[2024-11-05 13:20:05,232] - [BEST PARAMS ] - {'boosting_type': 'Plain', 'iterations': 683, 'learning_rate': 0.03, 'max_leaves': None, 'grow_policy': 'Depthwise', 'depth': 6, 'l2_leaf_reg': 30.39430513851218, 'model_size_reg': 70.64901962342054, 'od_wait': 100, 'bootstrap_type': 'Bernoulli', 'rsm': 0.7866218528265505, 'subsample': 0.46452464728155557, 'min_data_in_leaf': 44, 'one_hot_max_size': 10, 'auto_class_weights': 'SqrtBalanced', 'thread_count': 16, 'random_state': 77, 'verbose': False, 'allow_writing_files': False}\n",
      "[2024-11-05 13:20:05,233] - [    END     ] - Tuning CatBoostClassification\n",
      "[2024-11-05 13:20:05,233] - [   START    ] - Fitting CatBoostClassification\n",
      "[2024-11-05 13:20:05,277] - [    FIT     ] - CatBoostClassification fold 0\n",
      "[2024-11-05 13:20:20,425] - [    FIT     ] - CatBoostClassification fold 1\n",
      "[2024-11-05 13:20:35,638] - [    FIT     ] - CatBoostClassification fold 2\n",
      "[2024-11-05 13:20:50,753] - [    FIT     ] - CatBoostClassification fold 3\n",
      "[2024-11-05 13:21:06,837] - [    FIT     ] - CatBoostClassification fold 4\n",
      "[2024-11-05 13:21:22,170] - [    END     ] - Fitting CatBoostClassification\n",
      "[2024-11-05 13:21:26,248] - [   SCORE    ] - Train: 0.8398045969891614\n",
      "[2024-11-05 13:21:26,358] - [   SCORE    ] - OOF: 0.8024041541993816\n",
      "[2024-11-05 13:21:28,359] - [   SCORE    ] - Test: 0.7959585758054861\n",
      "[2024-11-05 13:21:28,361] - [   SCORE    ] - Overfit: 5.22 %\n",
      "[2024-11-05 13:21:29,009] - [    END     ] - Working with CatBoostClassification\n",
      "[2024-11-05 13:21:29,010] - [  NEW BEST  ] - CatBoostClassification. Best score: 0.7959585758054861 \n",
      "\n",
      "[2024-11-05 13:21:29,011] - [   MODEL    ] - 5 out of 15. LightGBMClassification\n",
      "[2024-11-05 13:21:29,012] - [   START    ] - Working with LightGBMClassification\n",
      "[2024-11-05 13:21:29,013] - [   START    ] - Tuning LightGBMClassification\n",
      "[2024-11-05 13:21:42,400] - [   OPTUNA   ] - Trial 0. New best score 0.7908633007768426 with parameters {'max_depth': 15, 'num_leaves': 333, 'min_data_in_leaf': 193, 'bagging_fraction': 0.5696572840513675, 'bagging_freq': 0, 'feature_fraction': 0.8728012348218951, 'lambda_l1': 3.2615093703448883, 'lambda_l2': 5.410678214759677, 'min_gain_to_split': 4.804703517360121, 'is_unbalance': True, 'num_iterations': 3}\n",
      "[2024-11-05 13:22:12,342] - [   OPTUNA   ] - Trial 4. New best score 0.8019827005170403 with parameters {'max_depth': 15, 'num_leaves': 403, 'min_data_in_leaf': 68, 'bagging_fraction': 0.6547415930774714, 'bagging_freq': 0, 'feature_fraction': 0.6533028137401558, 'lambda_l1': 9.397650349673574, 'lambda_l2': 3.6704287316054973, 'min_gain_to_split': 8.69549945365769, 'is_unbalance': False, 'num_iterations': 207}\n",
      "[2024-11-05 13:22:56,871] - [   OPTUNA   ] - Trial 9. New best score 0.8034532660486475 with parameters {'max_depth': 16, 'num_leaves': 292, 'min_data_in_leaf': 229, 'bagging_fraction': 0.6562391434885229, 'bagging_freq': 20, 'feature_fraction': 0.5281904260758478, 'lambda_l1': 2.883443963127228, 'lambda_l2': 2.7344279519228376, 'min_gain_to_split': 4.970014244484826, 'is_unbalance': False, 'num_iterations': 275}\n",
      "[2024-11-05 13:23:12,697] - [   OPTUNA   ] - Trial 10. New best score 0.8045879943065837 with parameters {'max_depth': 10, 'num_leaves': 16, 'min_data_in_leaf': 236, 'bagging_fraction': 0.7796447877156262, 'bagging_freq': 20, 'feature_fraction': 0.4185201248391479, 'lambda_l1': 0.27531893376737493, 'lambda_l2': 8.5246514699545, 'min_gain_to_split': 1.118233382362238, 'is_unbalance': False, 'num_iterations': 487}\n",
      "[2024-11-05 13:23:58,296] - [   OPTUNA   ] - Trial 12. New best score 0.8047957305125302 with parameters {'max_depth': 10, 'num_leaves': 19, 'min_data_in_leaf': 244, 'bagging_fraction': 0.7814201870423789, 'bagging_freq': 20, 'feature_fraction': 0.4061352712886796, 'lambda_l1': 0.10492393348607187, 'lambda_l2': 9.81196338149507, 'min_gain_to_split': 0.19765941566375267, 'is_unbalance': False, 'num_iterations': 487}\n",
      "[2024-11-05 13:24:52,152] - [   OPTUNA   ] - Trial 16. New best score 0.8048736383343366 with parameters {'max_depth': 8, 'num_leaves': 81, 'min_data_in_leaf': 248, 'bagging_fraction': 0.8396442962874514, 'bagging_freq': 10, 'feature_fraction': 0.45385476464476804, 'lambda_l1': 0.4432066523825522, 'lambda_l2': 8.267515878909585, 'min_gain_to_split': 3.022405136996065, 'is_unbalance': False, 'num_iterations': 273}\n",
      "[2024-11-05 13:25:54,197] - [   OPTUNA   ] - Trial 21. New best score 0.804978778238888 with parameters {'max_depth': 9, 'num_leaves': 50, 'min_data_in_leaf': 224, 'bagging_fraction': 0.778075207820297, 'bagging_freq': 20, 'feature_fraction': 0.41191103775032994, 'lambda_l1': 0.039335933108372745, 'lambda_l2': 8.045457095987555, 'min_gain_to_split': 1.8193352163220478, 'is_unbalance': False, 'num_iterations': 288}\n",
      "[2024-11-05 13:26:39,002] - [   OPTUNA   ] - Trial 24. New best score 0.8050618238680697 with parameters {'max_depth': 7, 'num_leaves': 85, 'min_data_in_leaf': 162, 'bagging_fraction': 0.9073942790005392, 'bagging_freq': 20, 'feature_fraction': 0.4632002607000075, 'lambda_l1': 0.9117760912120141, 'lambda_l2': 6.182177721979992, 'min_gain_to_split': 2.764898005468358, 'is_unbalance': False, 'num_iterations': 284}\n",
      "[2024-11-05 13:26:39,006] - [   OPTUNA   ] - 25 trials completed\n",
      "[2024-11-05 13:26:39,007] - [BEST PARAMS ] - {'objective_type': 'binary', 'boosting': 'gbdt', 'num_iterations': 284, 'max_depth': 7, 'learning_rate': 0.03, 'num_leaves': 85, 'min_data_in_leaf': 162, 'bagging_fraction': 0.9073942790005392, 'bagging_freq': 20, 'feature_fraction': 0.4632002607000075, 'early_stopping_round': 100, 'lambda_l1': 0.9117760912120141, 'lambda_l2': 6.182177721979992, 'min_gain_to_split': 2.764898005468358, 'num_threads': 16, 'random_state': 77, 'is_unbalance': False, 'num_classes': 1, 'verbose': -1}\n",
      "[2024-11-05 13:26:39,008] - [    END     ] - Tuning LightGBMClassification\n",
      "[2024-11-05 13:26:39,008] - [   START    ] - Fitting LightGBMClassification\n",
      "[2024-11-05 13:26:39,053] - [    FIT     ] - LightGBMClassification fold 0\n",
      "[2024-11-05 13:26:41,738] - [    FIT     ] - LightGBMClassification fold 1\n",
      "[2024-11-05 13:26:44,484] - [    FIT     ] - LightGBMClassification fold 2\n",
      "[2024-11-05 13:26:47,219] - [    FIT     ] - LightGBMClassification fold 3\n",
      "[2024-11-05 13:26:49,991] - [    FIT     ] - LightGBMClassification fold 4\n",
      "[2024-11-05 13:26:52,620] - [    END     ] - Fitting LightGBMClassification\n",
      "[2024-11-05 13:26:54,925] - [   SCORE    ] - Train: 0.8376847263698198\n",
      "[2024-11-05 13:26:55,036] - [   SCORE    ] - OOF: 0.80499919603869\n",
      "[2024-11-05 13:26:55,881] - [   SCORE    ] - Test: 0.7974371413248466\n",
      "[2024-11-05 13:26:55,882] - [   SCORE    ] - Overfit: 4.80 %\n",
      "[2024-11-05 13:26:56,532] - [    END     ] - Working with LightGBMClassification\n",
      "[2024-11-05 13:26:56,533] - [  NEW BEST  ] - LightGBMClassification. Best score: 0.7974371413248466 \n",
      "\n",
      "[2024-11-05 13:26:56,534] - [   MODEL    ] - 6 out of 15. XGBClassification\n",
      "[2024-11-05 13:26:56,535] - [   START    ] - Working with XGBClassification\n",
      "[2024-11-05 13:26:56,536] - [   START    ] - Tuning XGBClassification\n",
      "[2024-11-05 13:29:54,820] - [   OPTUNA   ] - Trial 0. New best score 0.7965722759103466 with parameters {'max_depth': 15, 'grow_policy': 'lossguide', 'max_leaves': 80, 'gamma': 1.7463909597920013, 'subsample': 0.8092018522328426, 'colsample_bytree': 0.39353584333103997, 'colsample_bylevel': 0.586961039328371, 'reg_lambda': 2.4023517586800605, 'reg_alpha': 5.454229255759049, 'min_child_weight': 8, 'class_weight': 'balanced', 'n_estimators': 1999}\n",
      "[2024-11-05 13:32:44,045] - [   OPTUNA   ] - 2 trials completed\n",
      "[2024-11-05 13:32:44,046] - [BEST PARAMS ] - {'objective': 'binary:logistic', 'n_estimators': 1999, 'learning_rate': 0.03, 'max_depth': 15, 'max_leaves': 80, 'grow_policy': 'lossguide', 'gamma': 1.7463909597920013, 'min_child_weight': 8, 'subsample': 0.8092018522328426, 'colsample_bytree': 0.39353584333103997, 'colsample_bylevel': 0.586961039328371, 'reg_lambda': 2.4023517586800605, 'reg_alpha': 5.454229255759049, 'enable_categorical': True, 'max_cat_to_onehot': 5, 'n_jobs': 16, 'random_state': 77, 'verbosity': 0, 'early_stopping_rounds': 100, 'class_weight': 'balanced'}\n",
      "[2024-11-05 13:32:44,047] - [    END     ] - Tuning XGBClassification\n",
      "[2024-11-05 13:32:44,048] - [   START    ] - Fitting XGBClassification\n",
      "[2024-11-05 13:32:44,165] - [    FIT     ] - XGBClassification fold 0\n",
      "[2024-11-05 13:33:20,653] - [    FIT     ] - XGBClassification fold 1\n",
      "[2024-11-05 13:33:56,392] - [    FIT     ] - XGBClassification fold 2\n",
      "[2024-11-05 13:34:34,183] - [    FIT     ] - XGBClassification fold 3\n",
      "[2024-11-05 13:35:11,267] - [    FIT     ] - XGBClassification fold 4\n",
      "[2024-11-05 13:35:48,655] - [    END     ] - Fitting XGBClassification\n",
      "[2024-11-05 13:35:55,440] - [   SCORE    ] - Train: 0.9907597215225552\n",
      "[2024-11-05 13:35:55,556] - [   SCORE    ] - OOF: 0.7965394453074761\n",
      "[2024-11-05 13:35:58,194] - [   SCORE    ] - Test: 0.7946211214996499\n",
      "[2024-11-05 13:35:58,195] - [   SCORE    ] - Overfit: 19.80 %\n",
      "[2024-11-05 13:35:58,855] - [    END     ] - Working with XGBClassification\n",
      "[2024-11-05 13:35:58,856] - [BEST  MODEL ] - LightGBMClassification. Best score: 0.7974371413248466 \n",
      "\n",
      "[2024-11-05 13:35:58,858] - [   MODEL    ] - 7 out of 15. TabularLama\n",
      "[2024-11-05 13:35:58,859] - [   START    ] - Working with TabularLama\n",
      "[2024-11-05 13:35:58,860] - [   START    ] - Fitting TabularLama\n",
      "[13:35:58] Stdout logging level is INFO.\n",
      "[13:35:58] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[13:35:58] Task: binary\n",
      "\n",
      "[13:35:58] Start automl preset with listed constraints:\n",
      "[13:35:58] - time: 600.00 seconds\n",
      "[13:35:58] - CPU: 16 cores\n",
      "[13:35:58] - memory: 16 GB\n",
      "\n",
      "[13:35:58] \u001b[1mTrain data shape: (300000, 68)\u001b[0m\n",
      "\n",
      "[13:36:09] Layer \u001b[1m1\u001b[0m train process start. Time left 589.65 secs\n",
      "[13:36:18] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[13:36:48] Time limit exceeded after calculating fold 2\n",
      "\n",
      "[13:36:48] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7853527497383272\u001b[0m\n",
      "[13:36:48] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[13:36:48] Time left 550.79 secs\n",
      "\n",
      "[13:36:55] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:37:03] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[13:37:39] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8021764143460709\u001b[0m\n",
      "[13:37:39] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:37:39] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n",
      "[13:37:47] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[13:37:47] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[13:38:21] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.7996377512490308\u001b[0m\n",
      "[13:38:21] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:38:21] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[13:38:53] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.800632645782833\u001b[0m\n",
      "[13:38:53] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:38:53] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 199.25 secs\n",
      "[13:42:13] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[13:42:13] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[13:43:09] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8007996586558364\u001b[0m\n",
      "[13:43:09] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:43:09] Time left 168.97 secs\n",
      "\n",
      "[13:43:09] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[13:43:09] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[13:43:10] Blending: optimization starts with equal weights and score \u001b[1m0.802901554295063\u001b[0m\n",
      "[13:43:14] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8038465997814006\u001b[0m, weights = \u001b[1m[0.         0.4214027  0.24168442 0.18963076 0.14728214]\u001b[0m\n",
      "[13:43:17] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8038494666933915\u001b[0m, weights = \u001b[1m[0.         0.42690697 0.23068194 0.2116571  0.13075401]\u001b[0m\n",
      "[13:43:21] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8038498356771544\u001b[0m, weights = \u001b[1m[0.         0.42047235 0.23242444 0.21455748 0.13254575]\u001b[0m\n",
      "[13:43:25] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8038499498864142\u001b[0m, weights = \u001b[1m[0.         0.42150873 0.23200879 0.21417378 0.13230872]\u001b[0m\n",
      "[13:43:29] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.8038499498864142\u001b[0m, weights = \u001b[1m[0.         0.42150873 0.23200879 0.21417378 0.13230872]\u001b[0m\n",
      "[13:43:29] Blending: no score update. Terminated\n",
      "\n",
      "[13:43:29] \u001b[1mAutoml preset training completed in 450.66 seconds\u001b[0m\n",
      "\n",
      "[13:43:29] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.42151 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.23201 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.21417 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.13231 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[2024-11-05 13:43:29,603] - [    END     ] - Fitting TabularLama\n",
      "[2024-11-05 13:43:36,480] - [   SCORE    ] - Train: 0.9002921643384418\n",
      "[2024-11-05 13:43:36,589] - [   SCORE    ] - OOF: 0.8038499498864142\n",
      "[2024-11-05 13:43:39,372] - [   SCORE    ] - Test: 0.7970247654326865\n",
      "[2024-11-05 13:43:39,373] - [   SCORE    ] - Overfit: 11.47 %\n",
      "[2024-11-05 13:43:39,804] - [    END     ] - Working with TabularLama\n",
      "[2024-11-05 13:43:39,806] - [BEST  MODEL ] - LightGBMClassification. Best score: 0.7974371413248466 \n",
      "\n",
      "[2024-11-05 13:43:39,806] - [   MODEL    ] - 8 out of 15. TabularLamaUtilized\n",
      "[2024-11-05 13:43:39,807] - [   START    ] - Working with TabularLamaUtilized\n",
      "[2024-11-05 13:43:39,808] - [   START    ] - Fitting TabularLamaUtilized\n",
      "[13:43:39] Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n",
      "[13:43:39] - time: 600.00 seconds\n",
      "[13:43:39] - CPU: 16 cores\n",
      "[13:43:39] - memory: 16 GB\n",
      "\n",
      "[13:43:39] \u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
      "\n",
      "[13:43:39] ==================================================\n",
      "[13:43:39] Start 0 automl preset configuration:\n",
      "[13:43:39] \u001b[1mconf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n",
      "[13:43:39] Stdout logging level is INFO.\n",
      "[13:43:39] Task: binary\n",
      "\n",
      "[13:43:39] Start automl preset with listed constraints:\n",
      "[13:43:39] - time: 600.00 seconds\n",
      "[13:43:39] - CPU: 16 cores\n",
      "[13:43:39] - memory: 16 GB\n",
      "\n",
      "[13:43:39] \u001b[1mTrain data shape: (300000, 68)\u001b[0m\n",
      "\n",
      "[13:43:50] Layer \u001b[1m1\u001b[0m train process start. Time left 589.73 secs\n",
      "[13:43:58] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[13:44:36] Time limit exceeded after calculating fold 3\n",
      "\n",
      "[13:44:36] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7866485066512336\u001b[0m\n",
      "[13:44:36] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[13:44:36] Time left 543.22 secs\n",
      "\n",
      "[13:44:44] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[13:45:21] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8018898601182235\u001b[0m\n",
      "[13:45:21] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:45:21] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n",
      "[13:45:29] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[13:45:29] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[13:46:03] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.7995256906007128\u001b[0m\n",
      "[13:46:03] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[13:46:03] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[13:46:34] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.800162378273215\u001b[0m\n",
      "[13:46:34] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:46:34] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 263.32 secs\n",
      "[13:51:05] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[13:51:05] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[13:52:17] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.8009827283125209\u001b[0m\n",
      "[13:52:17] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[13:52:17] Time left 82.29 secs\n",
      "\n",
      "[13:52:17] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[13:52:17] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[13:52:17] Blending: optimization starts with equal weights and score \u001b[1m0.8026362018582571\u001b[0m\n",
      "[13:52:21] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8037779698996719\u001b[0m, weights = \u001b[1m[0.         0.38148755 0.24425845 0.1257091  0.24854487]\u001b[0m\n",
      "[13:52:25] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8037838687480477\u001b[0m, weights = \u001b[1m[0.         0.39711317 0.23967226 0.09793623 0.2652784 ]\u001b[0m\n",
      "[13:52:29] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8037857956299761\u001b[0m, weights = \u001b[1m[0.         0.39903387 0.23957603 0.0789189  0.28247112]\u001b[0m\n",
      "[13:52:33] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8037862563605134\u001b[0m, weights = \u001b[1m[0.         0.40027192 0.23815034 0.07210372 0.28947404]\u001b[0m\n",
      "[13:52:37] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.8037866013842917\u001b[0m, weights = \u001b[1m[0.         0.39655215 0.2425277  0.07029586 0.29062432]\u001b[0m\n",
      "[13:52:37] \u001b[1mAutoml preset training completed in 537.21 seconds\u001b[0m\n",
      "\n",
      "[13:52:37] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.39655 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.24253 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.07030 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.29062 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[13:52:37] ==================================================\n",
      "[2024-11-05 13:52:37,092] - [    END     ] - Fitting TabularLamaUtilized\n",
      "[2024-11-05 13:52:44,133] - [   SCORE    ] - Train: 0.9027563801693252\n",
      "[2024-11-05 13:52:44,242] - [   SCORE    ] - OOF: 0.8037866013842917\n",
      "[2024-11-05 13:52:46,943] - [   SCORE    ] - Test: 0.7972431043984296\n",
      "[2024-11-05 13:52:46,944] - [   SCORE    ] - Overfit: 11.69 %\n",
      "[2024-11-05 13:52:47,368] - [    END     ] - Working with TabularLamaUtilized\n",
      "[2024-11-05 13:52:47,369] - [BEST  MODEL ] - LightGBMClassification. Best score: 0.7974371413248466 \n",
      "\n",
      "[2024-11-05 13:52:47,370] - [   MODEL    ] - 9 out of 15. TabularLamaNN_mlp\n",
      "[2024-11-05 13:52:47,371] - [   START    ] - Working with TabularLamaNN_mlp\n",
      "[2024-11-05 13:52:47,372] - [   START    ] - Fitting TabularLamaNN_mlp\n",
      "[13:52:47] Stdout logging level is INFO.\n",
      "[13:52:47] Task: binary\n",
      "\n",
      "[13:52:47] Start automl preset with listed constraints:\n",
      "[13:52:47] - time: 600.00 seconds\n",
      "[13:52:47] - CPU: 16 cores\n",
      "[13:52:47] - memory: 16 GB\n",
      "\n",
      "[13:52:47] \u001b[1mTrain data shape: (300000, 68)\u001b[0m\n",
      "\n",
      "[13:52:57] Layer \u001b[1m1\u001b[0m train process start. Time left 589.91 secs\n",
      "[13:53:01] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m ...\n",
      "[14:00:23] Time limit exceeded after calculating fold 1\n",
      "\n",
      "[14:00:23] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m finished. score = \u001b[1m0.7800989526172374\u001b[0m\n",
      "[14:00:23] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m fitting and predicting completed\n",
      "[14:00:23] Time left 144.15 secs\n",
      "\n",
      "[14:00:23] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[14:00:23] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:00:23] \u001b[1mAutoml preset training completed in 455.86 seconds\u001b[0m\n",
      "\n",
      "[14:00:23] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (2 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_mlp_0) \n",
      "\n",
      "[2024-11-05 14:00:23,283] - [    END     ] - Fitting TabularLamaNN_mlp\n",
      "[2024-11-05 14:00:30,726] - [   SCORE    ] - Train: 0.7813917426765764\n",
      "[2024-11-05 14:00:30,765] - [   SCORE    ] - OOF: 0.7800989526172374\n",
      "[2024-11-05 14:00:33,650] - [   SCORE    ] - Test: 0.7768517364104534\n",
      "[2024-11-05 14:00:33,652] - [   SCORE    ] - Overfit: 0.58 %\n",
      "[2024-11-05 14:00:33,826] - [    END     ] - Working with TabularLamaNN_mlp\n",
      "[2024-11-05 14:00:33,828] - [BEST  MODEL ] - LightGBMClassification. Best score: 0.7974371413248466 \n",
      "\n",
      "[2024-11-05 14:00:33,829] - [   MODEL    ] - 10 out of 15. TabularLamaNN_denselight\n",
      "[2024-11-05 14:00:33,830] - [   START    ] - Working with TabularLamaNN_denselight\n",
      "[2024-11-05 14:00:33,831] - [   START    ] - Fitting TabularLamaNN_denselight\n",
      "[14:00:33] Stdout logging level is INFO.\n",
      "[14:00:33] Task: binary\n",
      "\n",
      "[14:00:33] Start automl preset with listed constraints:\n",
      "[14:00:33] - time: 600.00 seconds\n",
      "[14:00:33] - CPU: 16 cores\n",
      "[14:00:33] - memory: 16 GB\n",
      "\n",
      "[14:00:33] \u001b[1mTrain data shape: (300000, 68)\u001b[0m\n",
      "\n",
      "[14:00:44] Layer \u001b[1m1\u001b[0m train process start. Time left 589.86 secs\n",
      "[14:00:48] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m ...\n",
      "[14:08:26] Time limit exceeded after calculating fold 1\n",
      "\n",
      "[14:08:26] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m finished. score = \u001b[1m0.7775525663775014\u001b[0m\n",
      "[14:08:26] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m fitting and predicting completed\n",
      "[14:08:26] Time left 127.37 secs\n",
      "\n",
      "[14:08:26] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[14:08:26] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:08:26] \u001b[1mAutoml preset training completed in 472.64 seconds\u001b[0m\n",
      "\n",
      "[14:08:26] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (2 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_denselight_0) \n",
      "\n",
      "[2024-11-05 14:08:26,520] - [    END     ] - Fitting TabularLamaNN_denselight\n",
      "[2024-11-05 14:08:33,974] - [   SCORE    ] - Train: 0.778576023688585\n",
      "[2024-11-05 14:08:34,014] - [   SCORE    ] - OOF: 0.7775525663775014\n",
      "[2024-11-05 14:08:36,914] - [   SCORE    ] - Test: 0.7745361140710456\n",
      "[2024-11-05 14:08:36,915] - [   SCORE    ] - Overfit: 0.52 %\n",
      "[2024-11-05 14:08:37,090] - [    END     ] - Working with TabularLamaNN_denselight\n",
      "[2024-11-05 14:08:37,092] - [BEST  MODEL ] - LightGBMClassification. Best score: 0.7974371413248466 \n",
      "\n",
      "[2024-11-05 14:08:37,092] - [   MODEL    ] - 11 out of 15. TabularLamaNN_dense\n",
      "[2024-11-05 14:08:37,093] - [   START    ] - Working with TabularLamaNN_dense\n",
      "[2024-11-05 14:08:37,094] - [   START    ] - Fitting TabularLamaNN_dense\n",
      "[14:08:37] Stdout logging level is INFO.\n",
      "[14:08:37] Task: binary\n",
      "\n",
      "[14:08:37] Start automl preset with listed constraints:\n",
      "[14:08:37] - time: 600.00 seconds\n",
      "[14:08:37] - CPU: 16 cores\n",
      "[14:08:37] - memory: 16 GB\n",
      "\n",
      "[14:08:37] \u001b[1mTrain data shape: (300000, 68)\u001b[0m\n",
      "\n",
      "[14:08:47] Layer \u001b[1m1\u001b[0m train process start. Time left 589.83 secs\n",
      "[14:08:51] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_dense_0\u001b[0m ...\n",
      "[14:16:14] Time limit exceeded after calculating fold 3\n",
      "\n",
      "[14:16:14] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_dense_0\u001b[0m finished. score = \u001b[1m0.7932409984553795\u001b[0m\n",
      "[14:16:14] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_dense_0\u001b[0m fitting and predicting completed\n",
      "[14:16:14] Time left 142.74 secs\n",
      "\n",
      "[14:16:14] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[14:16:14] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:16:14] \u001b[1mAutoml preset training completed in 457.26 seconds\u001b[0m\n",
      "\n",
      "[14:16:14] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (4 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_dense_0) \n",
      "\n",
      "[2024-11-05 14:16:14,407] - [    END     ] - Fitting TabularLamaNN_dense\n",
      "[2024-11-05 14:16:28,853] - [   SCORE    ] - Train: 0.8176721169846631\n",
      "[2024-11-05 14:16:28,937] - [   SCORE    ] - OOF: 0.7932409984553795\n",
      "[2024-11-05 14:16:35,003] - [   SCORE    ] - Test: 0.7888530150150797\n",
      "[2024-11-05 14:16:35,005] - [   SCORE    ] - Overfit: 3.52 %\n",
      "[2024-11-05 14:16:35,358] - [    END     ] - Working with TabularLamaNN_dense\n",
      "[2024-11-05 14:16:35,360] - [BEST  MODEL ] - LightGBMClassification. Best score: 0.7974371413248466 \n",
      "\n",
      "[2024-11-05 14:16:35,360] - [   MODEL    ] - 12 out of 15. TabularLamaNN_resnet\n",
      "[2024-11-05 14:16:35,361] - [   START    ] - Working with TabularLamaNN_resnet\n",
      "[2024-11-05 14:16:35,362] - [   START    ] - Fitting TabularLamaNN_resnet\n",
      "[14:16:35] Stdout logging level is INFO.\n",
      "[14:16:35] Task: binary\n",
      "\n",
      "[14:16:35] Start automl preset with listed constraints:\n",
      "[14:16:35] - time: 600.00 seconds\n",
      "[14:16:35] - CPU: 16 cores\n",
      "[14:16:35] - memory: 16 GB\n",
      "\n",
      "[14:16:35] \u001b[1mTrain data shape: (300000, 68)\u001b[0m\n",
      "\n",
      "[14:16:45] Layer \u001b[1m1\u001b[0m train process start. Time left 589.76 secs\n",
      "[14:16:50] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_resnet_0\u001b[0m ...\n",
      "[14:21:10] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_resnet_0\u001b[0m finished. score = \u001b[1m0.7945594694508984\u001b[0m\n",
      "[14:21:10] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_resnet_0\u001b[0m fitting and predicting completed\n",
      "[14:21:10] Time left 324.52 secs\n",
      "\n",
      "[14:21:10] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:21:10] \u001b[1mAutoml preset training completed in 275.49 seconds\u001b[0m\n",
      "\n",
      "[14:21:10] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_resnet_0) \n",
      "\n",
      "[2024-11-05 14:21:10,899] - [    END     ] - Fitting TabularLamaNN_resnet\n",
      "[2024-11-05 14:21:22,362] - [   SCORE    ] - Train: 0.808224691062674\n",
      "[2024-11-05 14:21:22,471] - [   SCORE    ] - OOF: 0.7945594694508984\n",
      "[2024-11-05 14:21:27,198] - [   SCORE    ] - Test: 0.7894076402853483\n",
      "[2024-11-05 14:21:27,199] - [   SCORE    ] - Overfit: 2.33 %\n",
      "[2024-11-05 14:21:27,631] - [    END     ] - Working with TabularLamaNN_resnet\n",
      "[2024-11-05 14:21:27,633] - [BEST  MODEL ] - LightGBMClassification. Best score: 0.7974371413248466 \n",
      "\n",
      "[2024-11-05 14:21:27,633] - [   MODEL    ] - 13 out of 15. TabularLamaNN_node\n",
      "[2024-11-05 14:21:27,634] - [   START    ] - Working with TabularLamaNN_node\n",
      "[2024-11-05 14:21:27,635] - [   START    ] - Fitting TabularLamaNN_node\n",
      "[14:21:27] Stdout logging level is INFO.\n",
      "[14:21:27] Task: binary\n",
      "\n",
      "[14:21:27] Start automl preset with listed constraints:\n",
      "[14:21:27] - time: 600.00 seconds\n",
      "[14:21:27] - CPU: 16 cores\n",
      "[14:21:27] - memory: 16 GB\n",
      "\n",
      "[14:21:27] \u001b[1mTrain data shape: (300000, 68)\u001b[0m\n",
      "\n",
      "[14:21:37] Layer \u001b[1m1\u001b[0m train process start. Time left 589.76 secs\n",
      "[14:21:42] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_node_0\u001b[0m ...\n",
      "[2024-11-05 15:21:52]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, X_test, y_test, categorical_features=categorical_features, save_models=False, save_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d3980-f98d-45c0-80a7-75bca410149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 15:30:53,139] - [   MODEL    ] - 1 out of 2. TabularLamaNN_autoint\n",
      "[2024-11-05 15:30:53,143] - [   START    ] - Working with TabularLamaNN_autoint\n",
      "[2024-11-05 15:30:53,144] - [   START    ] - Fitting TabularLamaNN_autoint\n",
      "[15:30:53] Stdout logging level is INFO.\n",
      "[15:30:53] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[15:30:53] Task: binary\n",
      "\n",
      "[15:30:53] Start automl preset with listed constraints:\n",
      "[15:30:53] - time: 600.00 seconds\n",
      "[15:30:53] - CPU: 16 cores\n",
      "[15:30:53] - memory: 16 GB\n",
      "\n",
      "[15:30:53] \u001b[1mTrain data shape: (300000, 68)\u001b[0m\n",
      "\n",
      "[15:31:03] Layer \u001b[1m1\u001b[0m train process start. Time left 589.58 secs\n",
      "[15:31:08] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_autoint_0\u001b[0m ...\n",
      "[16:01:56] Time limit exceeded after calculating fold 0\n",
      "\n",
      "[16:01:56] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_autoint_0\u001b[0m finished. score = \u001b[1m0.7997318221320568\u001b[0m\n",
      "[16:01:56] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_autoint_0\u001b[0m fitting and predicting completed\n",
      "[16:01:56] Time left -1263.36 secs\n",
      "\n",
      "[16:01:56] Time limit exceeded. Last level models will be blended and unused pipelines will be pruned.\n",
      "\n",
      "[16:01:56] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[16:01:56] \u001b[1mAutoml preset training completed in 1863.37 seconds\u001b[0m\n",
      "\n",
      "[16:01:56] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (1 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_autoint_0) \n",
      "\n",
      "[2024-11-05 16:01:56,590] - [    END     ] - Fitting TabularLamaNN_autoint\n",
      "[2024-11-05 16:02:18,805] - [   SCORE    ] - Train: 0.8159157377010589\n",
      "[2024-11-05 16:02:18,826] - [   SCORE    ] - OOF: 0.7997318221320568\n",
      "[2024-11-05 16:02:29,148] - [   SCORE    ] - Test: 0.790140049548296\n",
      "[2024-11-05 16:02:29,149] - [   SCORE    ] - Overfit: 3.16 %\n",
      "[2024-11-05 16:02:29,241] - [    END     ] - Working with TabularLamaNN_autoint\n",
      "[2024-11-05 16:02:29,242] - [  NEW BEST  ] - TabularLamaNN_autoint. Best score: 0.790140049548296 \n",
      "\n",
      "[2024-11-05 16:02:29,243] - [   MODEL    ] - 2 out of 2. TabularLamaNN_fttransformer\n",
      "[2024-11-05 16:02:29,244] - [   START    ] - Working with TabularLamaNN_fttransformer\n",
      "[2024-11-05 16:02:29,245] - [   START    ] - Fitting TabularLamaNN_fttransformer\n",
      "[16:02:29] Stdout logging level is INFO.\n",
      "[16:02:29] Task: binary\n",
      "\n",
      "[16:02:29] Start automl preset with listed constraints:\n",
      "[16:02:29] - time: 600.00 seconds\n",
      "[16:02:29] - CPU: 16 cores\n",
      "[16:02:29] - memory: 16 GB\n",
      "\n",
      "[16:02:29] \u001b[1mTrain data shape: (300000, 68)\u001b[0m\n",
      "\n",
      "[16:02:39] Layer \u001b[1m1\u001b[0m train process start. Time left 589.62 secs\n",
      "[16:02:44] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_fttransformer_0\u001b[0m ...\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, X_test, y_test, categorical_features=categorical_features, save_models=False, save_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e169fa-1c78-423c-9023-56ffb76af926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.automl.model.lightgbm.lightgbm.LightGBMClassification at 0x7f68c2e3d600>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85cc343-9ad1-4b07-90e8-e01b65ae3234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective_type': 'binary',\n",
       " 'boosting': 'gbdt',\n",
       " 'num_iterations': 284,\n",
       " 'max_depth': 7,\n",
       " 'learning_rate': 0.03,\n",
       " 'num_leaves': 85,\n",
       " 'min_data_in_leaf': 162,\n",
       " 'bagging_fraction': 0.9073942790005392,\n",
       " 'bagging_freq': 20,\n",
       " 'feature_fraction': 0.4632002607000075,\n",
       " 'early_stopping_round': 100,\n",
       " 'lambda_l1': 0.9117760912120141,\n",
       " 'lambda_l2': 6.182177721979992,\n",
       " 'min_gain_to_split': 2.764898005468358,\n",
       " 'num_threads': 16,\n",
       " 'random_state': 77,\n",
       " 'is_unbalance': False,\n",
       " 'num_classes': 1,\n",
       " 'verbose': -1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7162af-72b8-48df-b3f8-cd4d2cf66c91",
   "metadata": {},
   "source": [
    "## Inference best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22275e56-48fa-4f3d-9f0c-22efadfde763",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(DATA_PATH / \"test_preproc.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fb86a7e-0937-4109-ac13-3e124361377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"target\"] = model.best_model.predict(test[cfg[\"selected_features\"] + oe_cols + ohe_cols])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89a7bdaf-777a-44e1-99ce-24bb21ead2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id', 'target']].to_csv('lgmb_oe_ohe_cols_0805.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2056205-3190-4e56-9814-baf6e0ef51ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    171919.000000\n",
       "mean          0.059860\n",
       "std           0.067099\n",
       "min           0.001254\n",
       "25%           0.014259\n",
       "50%           0.031264\n",
       "75%           0.074442\n",
       "max           0.489716\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"target\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c27084-59e9-41e2-a619-8b83cd935aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_kernel",
   "language": "python",
   "name": "base_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
