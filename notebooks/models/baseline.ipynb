{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa3af6a-d478-4c0e-a715-21f3f6707c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../../automl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe599f1-83eb-4e6b-8115-f05eb659ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nlp' extra dependecy package 'gensim' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'gensim' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "[2024-11-02 14:00:47]\n",
      "/home/peter/venvs/base_venv/lib/python3.10/site-packages/lightautoml/ml_algo/dl_model.py:42: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n",
      "/home/peter/venvs/base_venv/lib/python3.10/site-packages/lightautoml/text/embed.py:22: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n",
      "/home/peter/venvs/base_venv/lib/python3.10/site-packages/lightautoml/text/dl_transformers.py:25: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.automl.model import AutoML\n",
    "from src.automl.model.metrics import RocAuc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4701a7-04a9-42cd-b472-d9ca1dce955d",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9446b478-3af7-4d89-8d52-0d16996734e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 77\n",
    "DATA_PATH = Path(\"../../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1cf46-0712-400d-b190-15de4b596d15",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62027f85-a9fa-42b4-b542-bff8f5b2334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(DATA_PATH / \"train_preproc.parquet\")\n",
    "df_train, _ = train_test_split(df_train, train_size=100_000, random_state=RANDOM_SEED, stratify=df_train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4bbaca-2b74-4177-88e4-4897ff07498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = df_train.columns[df_train.columns.str.startswith(\"OneHotEncoder\")].values.tolist()\n",
    "oe_cols = df_train.columns[df_train.columns.str.startswith(\"OrdinalEncoder\")].values.tolist()\n",
    "te_cols = df_train.columns[df_train.columns.str.startswith(\"MeanTargetEncoder\")].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979b253b-f611-498c-919d-11993f85a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ordinal encoded columns\n",
    "X_train, y_train = df_train.drop(columns=[\"id\", \"target\", \"smpl\"] + te_cols), df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1b1783-d2c1-4915-9e51-d418fa48431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.sample(n=50, axis=1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b00734-0a36-4487-93ed-c78e6833ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = np.intersect1d(X_train.columns, ohe_cols + oe_cols).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abfc9d-95b1-4eda-9665-251676ed3d8f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e9b58f-d170-4930-9bd9-e67ac8828af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoML(\"classification\", RocAuc(), n_jobs=8, random_state=RANDOM_SEED, tuning_timeout=60 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f4479c2-feee-44cc-9434-196bee461a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-02 14:00:51,974] - [   MODEL    ] - 1 out of 11. LightGBMClassification\n",
      "[2024-11-02 14:00:51,978] - [   START    ] - Working with LightGBMClassification\n",
      "[2024-11-02 14:00:51,981] - [   START    ] - Tuning LightGBMClassification\n",
      "[2024-11-02 14:00:59,190] - [   OPTUNA   ] - Trial 0. New best score 0.7662935345019797 with parameters {'max_depth': 15, 'num_leaves': 333, 'min_data_in_leaf': 193, 'bagging_fraction': 0.5696572840513675, 'bagging_freq': 0, 'feature_fraction': 0.8728012348218951, 'lambda_l1': 3.2615093703448883, 'lambda_l2': 5.410678214759677, 'min_gain_to_split': 4.804703517360121, 'is_unbalance': True, 'num_iterations': 2}\n",
      "[2024-11-02 14:01:11,310] - [   OPTUNA   ] - Trial 4. New best score 0.7775803316637012 with parameters {'max_depth': 15, 'num_leaves': 403, 'min_data_in_leaf': 68, 'bagging_fraction': 0.6547415930774714, 'bagging_freq': 0, 'feature_fraction': 0.6533028137401558, 'lambda_l1': 9.397650349673574, 'lambda_l2': 3.6704287316054973, 'min_gain_to_split': 8.69549945365769, 'is_unbalance': False, 'num_iterations': 188}\n",
      "[2024-11-02 14:01:46,127] - [   OPTUNA   ] - Trial 14. New best score 0.7781669262510131 with parameters {'max_depth': 6, 'num_leaves': 403, 'min_data_in_leaf': 69, 'bagging_fraction': 0.7176179393372786, 'bagging_freq': 10, 'feature_fraction': 0.5237363825897473, 'lambda_l1': 7.512996610885769, 'lambda_l2': 3.335422598193791, 'min_gain_to_split': 4.314795987873854, 'is_unbalance': False, 'num_iterations': 243}\n",
      "[2024-11-02 14:01:49,404] - [   OPTUNA   ] - Trial 15. New best score 0.7782952082253923 with parameters {'max_depth': 7, 'num_leaves': 504, 'min_data_in_leaf': 61, 'bagging_fraction': 0.7405142888816078, 'bagging_freq': 0, 'feature_fraction': 0.40154810698730276, 'lambda_l1': 7.926659731988809, 'lambda_l2': 7.058523516294928, 'min_gain_to_split': 2.598090614116048, 'is_unbalance': False, 'num_iterations': 187}\n",
      "[2024-11-02 14:02:03,730] - [   OPTUNA   ] - Trial 19. New best score 0.7783882041026313 with parameters {'max_depth': 8, 'num_leaves': 452, 'min_data_in_leaf': 39, 'bagging_fraction': 0.8002401700006144, 'bagging_freq': 0, 'feature_fraction': 0.4604813651951499, 'lambda_l1': 8.288801997918274, 'lambda_l2': 9.791082952399575, 'min_gain_to_split': 2.502109191457225, 'is_unbalance': False, 'num_iterations': 202}\n",
      "[2024-11-02 14:02:08,102] - [   OPTUNA   ] - Trial 20. New best score 0.7785586219448805 with parameters {'max_depth': 8, 'num_leaves': 458, 'min_data_in_leaf': 33, 'bagging_fraction': 0.7923152451506954, 'bagging_freq': 0, 'feature_fraction': 0.40176786796942443, 'lambda_l1': 8.657907748404119, 'lambda_l2': 9.56090183604257, 'min_gain_to_split': 1.4596537162183696, 'is_unbalance': False, 'num_iterations': 179}\n",
      "[2024-11-02 14:04:19,500] - [   OPTUNA   ] - Trial 53. New best score 0.7787474410167783 with parameters {'max_depth': 7, 'num_leaves': 399, 'min_data_in_leaf': 14, 'bagging_fraction': 0.7477694693398077, 'bagging_freq': 0, 'feature_fraction': 0.5154146733319176, 'lambda_l1': 7.342202963600106, 'lambda_l2': 9.518123877193103, 'min_gain_to_split': 2.9283251774928205, 'is_unbalance': False, 'num_iterations': 192}\n",
      "[2024-11-02 14:05:53,264] - [   OPTUNA   ] - 82 trials completed\n",
      "[2024-11-02 14:05:53,265] - [BEST PARAMS ] - {'objective_type': 'binary', 'boosting': 'gbdt', 'num_iterations': 192, 'max_depth': 7, 'learning_rate': 0.03, 'num_leaves': 399, 'min_data_in_leaf': 14, 'bagging_fraction': 0.7477694693398077, 'bagging_freq': 0, 'feature_fraction': 0.5154146733319176, 'early_stopping_round': 100, 'lambda_l1': 7.342202963600106, 'lambda_l2': 9.518123877193103, 'min_gain_to_split': 2.9283251774928205, 'num_threads': 8, 'random_state': 77, 'is_unbalance': False, 'num_classes': 1, 'verbose': -1}\n",
      "[2024-11-02 14:05:53,266] - [    END     ] - Tuning LightGBMClassification\n",
      "[2024-11-02 14:05:53,266] - [   START    ] - Fitting LightGBMClassification\n",
      "[2024-11-02 14:05:53,280] - [    FIT     ] - LightGBMClassification fold 0\n",
      "[2024-11-02 14:05:53,857] - [    FIT     ] - LightGBMClassification fold 1\n",
      "[2024-11-02 14:05:54,426] - [    FIT     ] - LightGBMClassification fold 2\n",
      "[2024-11-02 14:05:55,024] - [    FIT     ] - LightGBMClassification fold 3\n",
      "[2024-11-02 14:05:55,608] - [    FIT     ] - LightGBMClassification fold 4\n",
      "[2024-11-02 14:05:56,181] - [    END     ] - Fitting LightGBMClassification\n",
      "[2024-11-02 14:05:56,836] - [   SCORE    ] - Train: 0.7978481218730418\n",
      "[2024-11-02 14:05:56,869] - [   SCORE    ] - OOF: 0.7786065534652911\n",
      "[2024-11-02 14:05:56,871] - [    END     ] - Working with LightGBMClassification\n",
      "[2024-11-02 14:05:56,872] - [  NEW BEST  ] - LightGBMClassification. Best score: 0.7786065534652911 \n",
      "\n",
      "[2024-11-02 14:05:56,873] - [   MODEL    ] - 2 out of 11. XGBClassification\n",
      "[2024-11-02 14:05:56,873] - [   START    ] - Working with XGBClassification\n",
      "[2024-11-02 14:05:56,874] - [   START    ] - Tuning XGBClassification\n",
      "[2024-11-02 14:07:50,218] - [   OPTUNA   ] - Trial 0. New best score 0.759962601153169 with parameters {'max_depth': 15, 'grow_policy': 'lossguide', 'max_leaves': 80, 'gamma': 1.7463909597920013, 'subsample': 0.8092018522328426, 'colsample_bytree': 0.39353584333103997, 'colsample_bylevel': 0.586961039328371, 'reg_lambda': 2.4023517586800605, 'reg_alpha': 5.454229255759049, 'min_child_weight': 8, 'class_weight': 'balanced', 'n_estimators': 1998}\n",
      "[2024-11-02 14:09:29,343] - [   OPTUNA   ] - Trial 2. New best score 0.7756453045997849 with parameters {'max_depth': 2, 'grow_policy': 'depthwise', 'max_leaves': 86, 'gamma': 10.934206794318316, 'subsample': 0.49896373384210024, 'colsample_bytree': 0.13245517900261994, 'colsample_bylevel': 0.8406038721065547, 'reg_lambda': 2.7329268312998414, 'reg_alpha': 1.6898521927284171, 'min_child_weight': 13, 'class_weight': 'balanced', 'n_estimators': 1076}\n",
      "[2024-11-02 14:09:39,430] - [   OPTUNA   ] - Trial 3. New best score 0.7789405270490138 with parameters {'max_depth': 7, 'grow_policy': 'lossguide', 'max_leaves': 376, 'gamma': 6.244701166623061, 'subsample': 0.9052797153646481, 'colsample_bytree': 0.8049462356571017, 'colsample_bylevel': 0.3358003072010871, 'reg_lambda': 3.0948318615494275, 'reg_alpha': 1.294506297724709, 'min_child_weight': 8, 'class_weight': None, 'n_estimators': 257}\n",
      "[2024-11-02 14:11:32,888] - [   OPTUNA   ] - 6 trials completed\n",
      "[2024-11-02 14:11:32,889] - [BEST PARAMS ] - {'objective': 'binary:logistic', 'n_estimators': 257, 'learning_rate': 0.03, 'max_depth': 7, 'max_leaves': 376, 'grow_policy': 'lossguide', 'gamma': 6.244701166623061, 'min_child_weight': 8, 'subsample': 0.9052797153646481, 'colsample_bytree': 0.8049462356571017, 'colsample_bylevel': 0.3358003072010871, 'reg_lambda': 3.0948318615494275, 'reg_alpha': 1.294506297724709, 'enable_categorical': True, 'max_cat_to_onehot': 5, 'n_jobs': 8, 'random_state': 77, 'verbosity': 0, 'early_stopping_rounds': 100, 'class_weight': None}\n",
      "[2024-11-02 14:11:32,889] - [    END     ] - Tuning XGBClassification\n",
      "[2024-11-02 14:11:32,890] - [   START    ] - Fitting XGBClassification\n",
      "[2024-11-02 14:11:32,917] - [    FIT     ] - XGBClassification fold 0\n",
      "[2024-11-02 14:11:34,415] - [    FIT     ] - XGBClassification fold 1\n",
      "[2024-11-02 14:11:35,927] - [    FIT     ] - XGBClassification fold 2\n",
      "[2024-11-02 14:11:37,432] - [    FIT     ] - XGBClassification fold 3\n",
      "[2024-11-02 14:11:38,935] - [    FIT     ] - XGBClassification fold 4\n",
      "[2024-11-02 14:11:40,454] - [    END     ] - Fitting XGBClassification\n",
      "[2024-11-02 14:11:40,875] - [   SCORE    ] - Train: 0.8061648368080232\n",
      "[2024-11-02 14:11:40,909] - [   SCORE    ] - OOF: 0.7786878565479501\n",
      "[2024-11-02 14:11:40,913] - [    END     ] - Working with XGBClassification\n",
      "[2024-11-02 14:11:40,914] - [  NEW BEST  ] - XGBClassification. Best score: 0.7786878565479501 \n",
      "\n",
      "[2024-11-02 14:11:40,915] - [   MODEL    ] - 3 out of 11. TabularLama\n",
      "[2024-11-02 14:11:40,916] - [   START    ] - Working with TabularLama\n",
      "[2024-11-02 14:11:40,917] - [   START    ] - Fitting TabularLama\n",
      "[14:11:40] Stdout logging level is INFO.\n",
      "[14:11:40] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:11:40] Task: binary\n",
      "\n",
      "[14:11:40] Start automl preset with listed constraints:\n",
      "[14:11:40] - time: 600.00 seconds\n",
      "[14:11:40] - CPU: 8 cores\n",
      "[14:11:40] - memory: 16 GB\n",
      "\n",
      "[14:11:40] \u001b[1mTrain data shape: (100000, 51)\u001b[0m\n",
      "\n",
      "[14:11:50] Layer \u001b[1m1\u001b[0m train process start. Time left 590.71 secs\n",
      "[14:11:53] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:12:03] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7698424717076328\u001b[0m\n",
      "[14:12:03] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:12:03] Time left 577.84 secs\n",
      "\n",
      "[14:12:06] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:12:07] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:12:25] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7755720972717981\u001b[0m\n",
      "[14:12:25] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:12:25] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 59.53 secs\n",
      "[14:13:30] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:13:30] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:13:40] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.7781455109862283\u001b[0m\n",
      "[14:13:40] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:13:40] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:13:46] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.7759572970273845\u001b[0m\n",
      "[14:13:46] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:13:46] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:16:18] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:16:18] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:16:27] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.7763188826426581\u001b[0m\n",
      "[14:16:27] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:16:27] Time left 313.08 secs\n",
      "\n",
      "[14:16:27] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:16:27] Blending: optimization starts with equal weights and score \u001b[1m0.7782665400950907\u001b[0m\n",
      "[14:16:29] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.7789936843784117\u001b[0m, weights = \u001b[1m[0.         0.1226801  0.5197348  0.16716051 0.19042462]\u001b[0m\n",
      "[14:16:30] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.7790002070993147\u001b[0m, weights = \u001b[1m[0.         0.09604371 0.53551453 0.17223568 0.19620612]\u001b[0m\n",
      "[14:16:31] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.7790254931088771\u001b[0m, weights = \u001b[1m[0.05019432 0.10173065 0.53222674 0.1476498  0.16819856]\u001b[0m\n",
      "[14:16:32] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.7790283569839901\u001b[0m, weights = \u001b[1m[0.05033065 0.08481047 0.5481527  0.14805081 0.16865541]\u001b[0m\n",
      "[14:16:33] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.7790283578842966\u001b[0m, weights = \u001b[1m[0.05033067 0.0848105  0.5481525  0.14805087 0.16865547]\u001b[0m\n",
      "[14:16:33] \u001b[1mAutoml preset training completed in 292.79 seconds\u001b[0m\n",
      "\n",
      "[14:16:33] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.05033 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.08481 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.54815 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.14805 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.16866 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[2024-11-02 14:16:33,746] - [    END     ] - Fitting TabularLama\n",
      "[2024-11-02 14:16:36,723] - [   SCORE    ] - Train: 0.8177406083100498\n",
      "[2024-11-02 14:16:36,757] - [   SCORE    ] - OOF: 0.7790283578842966\n",
      "[2024-11-02 14:16:36,758] - [    END     ] - Working with TabularLama\n",
      "[2024-11-02 14:16:36,759] - [  NEW BEST  ] - TabularLama. Best score: 0.7790283578842966 \n",
      "\n",
      "[2024-11-02 14:16:36,759] - [   MODEL    ] - 4 out of 11. TabularLamaUtilized\n",
      "[2024-11-02 14:16:36,760] - [   START    ] - Working with TabularLamaUtilized\n",
      "[2024-11-02 14:16:36,760] - [   START    ] - Fitting TabularLamaUtilized\n",
      "[14:16:36] Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n",
      "[14:16:36] - time: 600.00 seconds\n",
      "[14:16:36] - CPU: 8 cores\n",
      "[14:16:36] - memory: 16 GB\n",
      "\n",
      "[14:16:36] \u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
      "\n",
      "[14:16:36] ==================================================\n",
      "[14:16:36] Start 0 automl preset configuration:\n",
      "[14:16:36] \u001b[1mconf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n",
      "[14:16:36] Stdout logging level is INFO.\n",
      "[14:16:36] Task: binary\n",
      "\n",
      "[14:16:36] Start automl preset with listed constraints:\n",
      "[14:16:36] - time: 600.00 seconds\n",
      "[14:16:36] - CPU: 8 cores\n",
      "[14:16:36] - memory: 16 GB\n",
      "\n",
      "[14:16:36] \u001b[1mTrain data shape: (100000, 51)\u001b[0m\n",
      "\n",
      "[14:16:46] Layer \u001b[1m1\u001b[0m train process start. Time left 590.60 secs\n",
      "[14:16:49] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[14:17:02] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7701873053190638\u001b[0m\n",
      "[14:17:02] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[14:17:02] Time left 574.27 secs\n",
      "\n",
      "[14:17:05] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[14:17:23] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7741675632459404\u001b[0m\n",
      "[14:17:23] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:17:23] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 66.93 secs\n",
      "[14:18:31] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[14:18:31] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[14:18:42] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.7788035594433049\u001b[0m\n",
      "[14:18:42] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[14:18:42] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[14:18:49] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.7754375194497725\u001b[0m\n",
      "[14:18:49] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:18:49] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[14:21:49] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[14:21:49] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[14:22:00] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.7765320176123872\u001b[0m\n",
      "[14:22:00] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[14:22:00] Time left 276.36 secs\n",
      "\n",
      "[14:22:00] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:22:00] Blending: optimization starts with equal weights and score \u001b[1m0.7784202251228755\u001b[0m\n",
      "[14:22:01] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.7793234729672049\u001b[0m, weights = \u001b[1m[0.         0.12117293 0.65756935 0.07197083 0.14928684]\u001b[0m\n",
      "[14:22:02] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.7793810529724596\u001b[0m, weights = \u001b[1m[0.06114594 0.16729265 0.62352276 0.05555236 0.09248632]\u001b[0m\n",
      "[14:22:04] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.7793824214384049\u001b[0m, weights = \u001b[1m[0.06155945 0.179876   0.60885006 0.05660276 0.09311178]\u001b[0m\n",
      "[14:22:05] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.7793830111391902\u001b[0m, weights = \u001b[1m[0.06414452 0.17607641 0.613227   0.05540712 0.09114495]\u001b[0m\n",
      "[14:22:06] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.7793830678585025\u001b[0m, weights = \u001b[1m[0.06363909 0.18256858 0.6083951  0.05497053 0.09042677]\u001b[0m\n",
      "[14:22:06] \u001b[1mAutoml preset training completed in 329.59 seconds\u001b[0m\n",
      "\n",
      "[14:22:06] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.06364 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.18257 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.60840 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) +\n",
      "\t 0.05497 * (5 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) +\n",
      "\t 0.09043 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "[14:22:06] ==================================================\n",
      "[2024-11-02 14:22:06,406] - [    END     ] - Fitting TabularLamaUtilized\n",
      "[2024-11-02 14:22:10,239] - [   SCORE    ] - Train: 0.8301247047363665\n",
      "[2024-11-02 14:22:10,272] - [   SCORE    ] - OOF: 0.7793830678585025\n",
      "[2024-11-02 14:22:10,275] - [    END     ] - Working with TabularLamaUtilized\n",
      "[2024-11-02 14:22:10,276] - [  NEW BEST  ] - TabularLamaUtilized. Best score: 0.7793830678585025 \n",
      "\n",
      "[2024-11-02 14:22:10,277] - [   MODEL    ] - 5 out of 11. TabularLamaNN_mlp\n",
      "[2024-11-02 14:22:10,277] - [   START    ] - Working with TabularLamaNN_mlp\n",
      "[2024-11-02 14:22:10,278] - [   START    ] - Fitting TabularLamaNN_mlp\n",
      "[14:22:10] Stdout logging level is INFO.\n",
      "[14:22:10] Task: binary\n",
      "\n",
      "[14:22:10] Start automl preset with listed constraints:\n",
      "[14:22:10] - time: 600.00 seconds\n",
      "[14:22:10] - CPU: 8 cores\n",
      "[14:22:10] - memory: 16 GB\n",
      "\n",
      "[14:22:10] \u001b[1mTrain data shape: (100000, 51)\u001b[0m\n",
      "\n",
      "[14:22:19] Layer \u001b[1m1\u001b[0m train process start. Time left 590.87 secs\n",
      "[14:22:20] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m ...\n",
      "[14:28:18] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m finished. score = \u001b[1m0.7595685929834328\u001b[0m\n",
      "[14:28:18] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m fitting and predicting completed\n",
      "[14:28:18] Time left 231.98 secs\n",
      "\n",
      "[14:28:18] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:28:18] \u001b[1mAutoml preset training completed in 368.02 seconds\u001b[0m\n",
      "\n",
      "[14:28:18] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_mlp_0) \n",
      "\n",
      "[2024-11-02 14:28:18,335] - [    END     ] - Fitting TabularLamaNN_mlp\n",
      "[2024-11-02 14:28:23,371] - [   SCORE    ] - Train: 0.7660608015973656\n",
      "[2024-11-02 14:28:23,410] - [   SCORE    ] - OOF: 0.7595685929834328\n",
      "[2024-11-02 14:28:23,414] - [    END     ] - Working with TabularLamaNN_mlp\n",
      "[2024-11-02 14:28:23,415] - [BEST  MODEL ] - TabularLamaUtilized. Best score: 0.7793830678585025 \n",
      "\n",
      "[2024-11-02 14:28:23,417] - [   MODEL    ] - 6 out of 11. TabularLamaNN_denselight\n",
      "[2024-11-02 14:28:23,418] - [   START    ] - Working with TabularLamaNN_denselight\n",
      "[2024-11-02 14:28:23,419] - [   START    ] - Fitting TabularLamaNN_denselight\n",
      "[14:28:23] Stdout logging level is INFO.\n",
      "[14:28:23] Task: binary\n",
      "\n",
      "[14:28:23] Start automl preset with listed constraints:\n",
      "[14:28:23] - time: 600.00 seconds\n",
      "[14:28:23] - CPU: 8 cores\n",
      "[14:28:23] - memory: 16 GB\n",
      "\n",
      "[14:28:23] \u001b[1mTrain data shape: (100000, 51)\u001b[0m\n",
      "\n",
      "[14:28:32] Layer \u001b[1m1\u001b[0m train process start. Time left 590.72 secs\n",
      "[14:28:34] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m ...\n",
      "[14:34:28] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m finished. score = \u001b[1m0.7686155879694241\u001b[0m\n",
      "[14:34:28] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m fitting and predicting completed\n",
      "[14:34:28] Time left 234.64 secs\n",
      "\n",
      "[14:34:28] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:34:28] \u001b[1mAutoml preset training completed in 365.36 seconds\u001b[0m\n",
      "\n",
      "[14:34:28] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_denselight_0) \n",
      "\n",
      "[2024-11-02 14:34:28,815] - [    END     ] - Fitting TabularLamaNN_denselight\n",
      "[2024-11-02 14:34:33,867] - [   SCORE    ] - Train: 0.7702009008481673\n",
      "[2024-11-02 14:34:33,900] - [   SCORE    ] - OOF: 0.7686155879694241\n",
      "[2024-11-02 14:34:33,902] - [    END     ] - Working with TabularLamaNN_denselight\n",
      "[2024-11-02 14:34:33,903] - [BEST  MODEL ] - TabularLamaUtilized. Best score: 0.7793830678585025 \n",
      "\n",
      "[2024-11-02 14:34:33,903] - [   MODEL    ] - 7 out of 11. TabularLamaNN_dense\n",
      "[2024-11-02 14:34:33,904] - [   START    ] - Working with TabularLamaNN_dense\n",
      "[2024-11-02 14:34:33,904] - [   START    ] - Fitting TabularLamaNN_dense\n",
      "[14:34:33] Stdout logging level is INFO.\n",
      "[14:34:33] Task: binary\n",
      "\n",
      "[14:34:33] Start automl preset with listed constraints:\n",
      "[14:34:33] - time: 600.00 seconds\n",
      "[14:34:33] - CPU: 8 cores\n",
      "[14:34:33] - memory: 16 GB\n",
      "\n",
      "[14:34:33] \u001b[1mTrain data shape: (100000, 51)\u001b[0m\n",
      "\n",
      "[14:34:43] Layer \u001b[1m1\u001b[0m train process start. Time left 590.65 secs\n",
      "[14:34:44] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_dense_0\u001b[0m ...\n",
      "[14:38:09] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_dense_0\u001b[0m finished. score = \u001b[1m0.7639289387253456\u001b[0m\n",
      "[14:38:09] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_dense_0\u001b[0m fitting and predicting completed\n",
      "[14:38:09] Time left 384.85 secs\n",
      "\n",
      "[14:38:09] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:38:09] \u001b[1mAutoml preset training completed in 215.16 seconds\u001b[0m\n",
      "\n",
      "[14:38:09] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_dense_0) \n",
      "\n",
      "[2024-11-02 14:38:09,094] - [    END     ] - Fitting TabularLamaNN_dense\n",
      "[2024-11-02 14:38:16,588] - [   SCORE    ] - Train: 0.8075570186281131\n",
      "[2024-11-02 14:38:16,623] - [   SCORE    ] - OOF: 0.7639289387253456\n",
      "[2024-11-02 14:38:16,625] - [    END     ] - Working with TabularLamaNN_dense\n",
      "[2024-11-02 14:38:16,626] - [BEST  MODEL ] - TabularLamaUtilized. Best score: 0.7793830678585025 \n",
      "\n",
      "[2024-11-02 14:38:16,627] - [   MODEL    ] - 8 out of 11. TabularLamaNN_resnet\n",
      "[2024-11-02 14:38:16,628] - [   START    ] - Working with TabularLamaNN_resnet\n",
      "[2024-11-02 14:38:16,629] - [   START    ] - Fitting TabularLamaNN_resnet\n",
      "[14:38:16] Stdout logging level is INFO.\n",
      "[14:38:16] Task: binary\n",
      "\n",
      "[14:38:16] Start automl preset with listed constraints:\n",
      "[14:38:16] - time: 600.00 seconds\n",
      "[14:38:16] - CPU: 8 cores\n",
      "[14:38:16] - memory: 16 GB\n",
      "\n",
      "[14:38:16] \u001b[1mTrain data shape: (100000, 51)\u001b[0m\n",
      "\n",
      "[14:38:25] Layer \u001b[1m1\u001b[0m train process start. Time left 590.72 secs\n",
      "[14:38:27] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_resnet_0\u001b[0m ...\n",
      "[14:39:35] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_resnet_0\u001b[0m finished. score = \u001b[1m0.7694864499867939\u001b[0m\n",
      "[14:39:35] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_resnet_0\u001b[0m fitting and predicting completed\n",
      "[14:39:35] Time left 521.49 secs\n",
      "\n",
      "[14:39:35] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:39:35] \u001b[1mAutoml preset training completed in 78.51 seconds\u001b[0m\n",
      "\n",
      "[14:39:35] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_resnet_0) \n",
      "\n",
      "[2024-11-02 14:39:35,177] - [    END     ] - Fitting TabularLamaNN_resnet\n",
      "[2024-11-02 14:39:40,653] - [   SCORE    ] - Train: 0.7806454389797111\n",
      "[2024-11-02 14:39:40,691] - [   SCORE    ] - OOF: 0.7694864499867939\n",
      "[2024-11-02 14:39:40,693] - [    END     ] - Working with TabularLamaNN_resnet\n",
      "[2024-11-02 14:39:40,694] - [BEST  MODEL ] - TabularLamaUtilized. Best score: 0.7793830678585025 \n",
      "\n",
      "[2024-11-02 14:39:40,695] - [   MODEL    ] - 9 out of 11. TabularLamaNN_node\n",
      "[2024-11-02 14:39:40,696] - [   START    ] - Working with TabularLamaNN_node\n",
      "[2024-11-02 14:39:40,697] - [   START    ] - Fitting TabularLamaNN_node\n",
      "[14:39:40] Stdout logging level is INFO.\n",
      "[14:39:40] Task: binary\n",
      "\n",
      "[14:39:40] Start automl preset with listed constraints:\n",
      "[14:39:40] - time: 600.00 seconds\n",
      "[14:39:40] - CPU: 8 cores\n",
      "[14:39:40] - memory: 16 GB\n",
      "\n",
      "[14:39:40] \u001b[1mTrain data shape: (100000, 51)\u001b[0m\n",
      "\n",
      "[14:39:50] Layer \u001b[1m1\u001b[0m train process start. Time left 590.59 secs\n",
      "[14:39:51] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_node_0\u001b[0m ...\n",
      "[2024-11-02 14:55:48]\n",
      "Exception ignored in: <generator object BatchSampler.__iter__ at 0x7f73982d9b60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/peter/venvs/base_venv/lib/python3.10/site-packages/torch/utils/data/sampler.py\", line 248, in __iter__\n",
      "    yield batch\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, categorical_features=categorical_features, save_models=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22275e56-48fa-4f3d-9f0c-22efadfde763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_kernel",
   "language": "python",
   "name": "base_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
